{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with RecursiveUrlLoader (discarded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_community.document_loaders import RecursiveUrlLoader\n",
    "\n",
    "# Extracts text content from HTML, removing extra newlines and formatting it for readability.\n",
    "def bs4_extractor(html: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts clean text from the given HTML content, removing extra newlines for better readability.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n",
    "\n",
    "# Extracts the HTML content, preserving text and <img> tags, and placing the image in the text where found.\n",
    "def extract_text_with_images(html: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts both text and <img> tags from the given HTML content, placing images within the text where found.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    \n",
    "    output = \"\"\n",
    "    for element in soup.descendants:\n",
    "        if element.name not in ['script', 'style'] and isinstance(element, str):\n",
    "            output += element.strip() + \"\\n\\n\"\n",
    "        elif element.name == \"img\":\n",
    "            img_tag = f'<img src=\"{element.get(\"src\")}\", data-src=\"{element.get(\"data-src\")}\", alt=\"{element.get(\"alt\")}\", width=\"{element.get(\"width\")}\", height=\"{element.get(\"height\")}\">\\n\\n'\n",
    "            output += img_tag\n",
    "    \n",
    "    return re.sub(r\"\\n\\n+\", \"\\n\", output).strip()\n",
    "\n",
    "# Saves the given content to a file with the specified filename.\n",
    "def save_to_file(content: str, filename: str) -> None:\n",
    "    \"\"\"\n",
    "    Saves the content to a file with the given filename.\n",
    "    \"\"\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(content)\n",
    "\n",
    "# Process the loaded content and save the results\n",
    "def process_html_content(html_content: str, doc_i) -> None:\n",
    "    \"\"\"\n",
    "    Processes the HTML content by extracting text and images, and saves the results into files.\n",
    "    It also extracts the title and uses it to name the saved files.\n",
    "    \"\"\"\n",
    "    # Use BeautifulSoup to extract the title\n",
    "    soup = BeautifulSoup(html_content, \"lxml\")\n",
    "    title = soup.title.string if soup.title else \"untitled\"\n",
    "    \n",
    "    # Clean title for valid filename usage\n",
    "    safe_title = re.sub(r'[\\/:*?\"<>|]', \"_\", title)\n",
    "    \n",
    "    # Extract text content\n",
    "    extracted_text = bs4_extractor(html_content)\n",
    "    \n",
    "    # Extract text with images\n",
    "    text_with_images = extract_text_with_images(html_content)\n",
    "    \n",
    "    # Save files using the title as part of the filename\n",
    "    save_to_file(html_content, f\"{safe_title}[{doc_i}]_original_html.txt\")\n",
    "    save_to_file(extracted_text, f\"{safe_title}[{doc_i}]_text.txt\")\n",
    "    save_to_file(text_with_images, f\"{safe_title}[{doc_i}]_text_with_images.txt\")\n",
    "    \n",
    "    print(f\"Files saved with the title '{safe_title}'\")\n",
    "\n",
    "# Define the URL to be processed\n",
    "currenturl = \"https://www.gamersky.com/z/bmwukong/1314156_195585/\"\n",
    "\n",
    "# Use RecursiveUrlLoader without an extractor to get the original HTML content\n",
    "loader_html = RecursiveUrlLoader(\n",
    "    currenturl,\n",
    "    max_depth=10,\n",
    "    use_async=False,\n",
    "    extractor=None,  # No extractor here to get the original HTML\n",
    "    metadata_extractor=None,\n",
    "    exclude_dirs=(),\n",
    "    timeout=10,\n",
    "    check_response_status=True,\n",
    "    continue_on_failure=True,\n",
    "    prevent_outside=True,\n",
    "    base_url=None,\n",
    ")\n",
    "\n",
    "# Load the original HTML content\n",
    "docs_html = loader_html.load()\n",
    "\n",
    "# Process all documents loaded from RecursiveUrlLoader\n",
    "if docs_html and len(docs_html) > 0:\n",
    "    for i, doc in enumerate(docs_html):\n",
    "        html_content = doc.page_content\n",
    "        \n",
    "        # Optional: Include document index as part of the filename to differentiate files\n",
    "        print(f\"Processing document {i + 1}/{len(docs_html)}\")\n",
    "\n",
    "        # Process the HTML content by extracting text and images for each document\n",
    "        process_html_content(html_content, i)\n",
    "else:\n",
    "    print(\"Failed to load any content from the URL.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "\n",
    "display(Markdown(html_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import RecursiveUrlLoader\n",
    "\n",
    "# 定义要抓取的初始 URL\n",
    "currenturl = \"https://www.gamersky.com/z/bmwukong/\"\n",
    "\n",
    "# 配置 RecursiveUrlLoader\n",
    "loader = RecursiveUrlLoader(\n",
    "    currenturl,\n",
    "    max_depth=2,  # 设置递归抓取深度，例如 3 表示抓取当前页面及其链接的两级页面\n",
    "    use_async=False,  # 是否异步抓取\n",
    "    extractor=None,  # 提取器设为 None 以获取原始 HTML\n",
    "    metadata_extractor=None,  # 不使用元数据提取器\n",
    "    exclude_dirs=(),  # 可选，排除不需要抓取的目录\n",
    "    timeout=5,  # 每个页面的抓取超时时间\n",
    "    check_response_status=True,  # 是否检查 HTTP 响应状态码\n",
    "    continue_on_failure=True,  # 是否在遇到错误时继续抓取\n",
    "    prevent_outside=False,  # 防止抓取超出指定 URL 域名或目录的链接\n",
    "    base_url=currenturl,  # 确保只抓取从这个 URL 开始的页面\n",
    ")\n",
    "\n",
    "# 加载文档，返回一个包含所有递归抓取到页面的列表\n",
    "docs = loader.load()\n",
    "\n",
    "# 处理抓取到的文档\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"\\n-----------------\")\n",
    "    print(f\"Document {i+1}:\")  \n",
    "    print(doc.page_content.__len__())  # 输出每个文档的内容\n",
    "    print(doc.metadata)  # 输出每个文档的内容\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Complete page scrwling and data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from lxml import html, etree\n",
    "from urllib.parse import urljoin\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm  # Progress bar\n",
    "import json\n",
    "import urllib.request\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI \n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from io import BytesIO\n",
    "import base64\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv()) \n",
    "from langchain_together import ChatTogether\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ddd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling:   0%|          | 0/100 [00:00<?, ?it/s]/var/folders/03/4yvvy5rx6c97ntf957y4f7fw0000gn/T/ipykernel_73526/3852504055.py:226: FutureWarning: The behavior of this method will change in future versions. Use specific 'len(elem)' or 'elem is not None' test instead.\n",
      "  if tree:\n",
      "Crawling: 125653it [2:07:32, 16.42it/s]                   \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Log function to save logs to a file with date and time\n",
    "def log_message(message, filename=\"docs/mmgamerag.log\"):\n",
    "    \"\"\"\n",
    "    Saves the provided log message to a file with the current date and time.\n",
    "    \"\"\"\n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    from datetime import date\n",
    "    current_date = date.today()\n",
    "    filename=f\"docs/mmgamerag_{current_date}.log\"\n",
    "\n",
    "    with open(filename, \"a\", encoding=\"utf-8\") as log_file:\n",
    "        log_file.write(f\"[{current_time}] {message}\\n\")\n",
    "\n",
    "\n",
    "# Base64 encode the image\n",
    "def get_base64_encoded_image(image_url):\n",
    "    \"\"\"\n",
    "    Fetches the image from the given URL and returns its Base64 encoded string.\n",
    "    \"\"\"\n",
    "    return ''  # Temporarily returning an empty string for base64\n",
    "\n",
    "# Save content to file, for back up only, structured data is saved in JSON file.\n",
    "def save_content_to_file(content, filename):\n",
    "    \"\"\"\n",
    "    Saves the provided content to a file.\n",
    "    \"\"\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(content)\n",
    "    log_message(f\"Content successfully saved to {filename}\")\n",
    "\n",
    "def save_image_to_file(img_src):\n",
    "    \"\"\"\n",
    "    Save the image from the provided URL to a specified directory with a safe filename.\n",
    "    \n",
    "    Parameters:\n",
    "    img_src (str): The URL of the image to be saved.\n",
    "    \"\"\"\n",
    "    # Clean the URL to make it filename-safe\n",
    "    filename_safe_url = img_src.replace(\":\", \"=\").replace(\"/\", \"|\")\n",
    "    \n",
    "    # Specify the save path\n",
    "    save_directory = \"docs/rawdata/img\"\n",
    "    os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "    # Define the filename\n",
    "    filename = os.path.join(save_directory, f\"{filename_safe_url}\")\n",
    "    \n",
    "    # Download and save the image\n",
    "    urllib.request.urlretrieve(img_src, filename)\n",
    "    log_message(f\"Image saved as: {filename}\")\n",
    "\n",
    "\n",
    "# Save content to JSON file in a specified format\n",
    "def save_data_json_with_format(content, filename):\n",
    "    \"\"\"\n",
    "    Saves the provided content to a JSON file with specified indentation format.\n",
    "    Ensures the content is appended correctly to an existing JSON array.\n",
    "    \"\"\"\n",
    "    # Check if the file exists and load its content if it does\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'r', encoding=\"utf-8\") as json_file:\n",
    "            try:\n",
    "                existing_data = json.load(json_file)\n",
    "            except json.JSONDecodeError:\n",
    "                existing_data = []\n",
    "    else:\n",
    "        existing_data = []\n",
    "    \n",
    "    # Append new content to the existing data\n",
    "    existing_data.extend(content)\n",
    "    \n",
    "    # Save the updated data back to the file\n",
    "    with open(filename, 'w', encoding=\"utf-8\") as json_file:\n",
    "        json.dump(existing_data, json_file, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    log_message(f\"JSON content successfully saved to {filename}\")   \n",
    "\n",
    "# Extract image description from the image\n",
    "def get_image_description(image_src, content_before_image_str, content_after_image_str):\n",
    "    image_description=''\n",
    "    # return image_description\n",
    "\n",
    "    # imgdesllm = ChatOpenAI(name=\"image_des_llm\", model=\"gpt-4o-mini\")\n",
    "\n",
    "    imgdesllm = ChatNVIDIA(\n",
    "    model=\"meta/llama-3.2-90b-vision-instruct\",\n",
    "    api_key=\"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\"\n",
    "    )\n",
    "\n",
    "    # imgdesllm = ChatTogether(\n",
    "    # model=\"meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo\"\n",
    "    # # other params...\n",
    "    # )                 # 11b is not good enough\n",
    "\n",
    "    # ~~~~~~~~~~~Transfer image src to base64 and then send to llm for description handling faster.\n",
    "    # Clean the URL to make it filename-safe\n",
    "    filename_safe_url = image_src.replace(\":\", \"=\").replace(\"/\", \"|\")\n",
    "    filename_safe_url = 'docs/rawdata/img/' + filename_safe_url\n",
    "\n",
    "    if not os.path.exists(filename_safe_url):\n",
    "        log_message(f\"File {filename_safe_url} does not exist.\")\n",
    "        return\n",
    "\n",
    "    from PIL import Image\n",
    "    image_open = Image.open(filename_safe_url)\n",
    "\n",
    "    # Convert image to base64\n",
    "    buffered = BytesIO()\n",
    "\n",
    "    # Check the image format and save accordingly\n",
    "    if image_open.format == \"GIF\":\n",
    "        image_open.save(buffered, format=\"GIF\")\n",
    "    elif image_open.format == \"PNG\":\n",
    "        image_open.save(buffered, format=\"PNG\")\n",
    "    elif image_open.format == \"BMP\":\n",
    "        image_open.save(buffered, format=\"BMP\")\n",
    "    elif image_open.format == \"TIFF\":\n",
    "        image_open.save(buffered, format=\"TIFF\")\n",
    "    elif image_open.format == \"WEBP\":\n",
    "        image_open.save(buffered, format=\"WEBP\")\n",
    "    elif image_open.format == \"ICO\":\n",
    "        image_open.save(buffered, format=\"ICO\")\n",
    "    else:\n",
    "        # Default to JPEG if the format is not recognized\n",
    "        image_open.save(buffered, format=\"JPEG\")\n",
    "\n",
    "    img_base64 = base64.b64encode(buffered.getvalue()).decode()\n",
    "    # ~~~~~~~~~~~\n",
    "\n",
    "    message = HumanMessage(\n",
    "    content=[\n",
    "        {\"type\": \"text\", \"text\": f\"用300字以内的中文描述这张图片（以下简称此图）的内容。并将此图的上文和下文中的内容总结到此图的描述中，越靠近此图的上文和下文内容越重要越需要重点总结。此图的上文：\\n{content_before_image_str}\\n此图的下文：\\n{content_after_image_str}。\\n\\n严格遵循以下格式： 此图的上文提到..., 此图的描述为..., 此图的下文提到...。  \\n\\n\"},\n",
    "        # {\"type\": \"image_url\", \"image_url\": {\"url\": image_src}},\n",
    "        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"}},\n",
    "        ],\n",
    "    )\n",
    "    img_response = imgdesllm.invoke([message])\n",
    "    # print(f'\\n------------------------\\n{img_response.content}')\n",
    "    image_description=img_response.content\n",
    "\n",
    "    return image_description\n",
    "\n",
    "def get_all_image_description(file_path):\n",
    "    \"\"\"\n",
    "    Reads the JSON file, updates the image description for each item, and writes the updated data back to a temporary file\n",
    "    in batches. After processing all items, the temporary file is renamed to replace the original file.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): The path to the JSON file.\n",
    "    \"\"\"\n",
    "    # Log the start of the process\n",
    "    log_message(f\"=== Starting to get image description from file: {file_path} === \")\n",
    "\n",
    "    # Read the JSON file\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "        log_message(f\"Successfully loaded the JSON file: {file_path}\")\n",
    "    except Exception as e:\n",
    "        log_message(f\"Error loading the JSON file: {file_path}. Error: {e}\")\n",
    "        return\n",
    "    \n",
    "    temp_file_path = file_path + \".tmp\"\n",
    "    batch_size = 20  # Set batch size to 100\n",
    "\n",
    "    # Process the data in batches of 100 items\n",
    "    # for index, item in enumerate(tqdm(data, desc=\"Processing images\")):\n",
    "    for index, item in enumerate(tqdm(data[0:19754], desc=\"Processing images\")):\n",
    "        try:\n",
    "            time.sleep(0.05)\n",
    "            # Extract relevant fields\n",
    "            image_src = item.get('src', '')\n",
    "            content_before_image_str = item.get('content_before_image', '')\n",
    "            content_after_image_str = item.get('content_after_image', '')\n",
    "            image_description_in_file = item.get('image_description', '')\n",
    "            if image_description_in_file != \"\":\n",
    "                continue\n",
    "\n",
    "            \n",
    "            # Call the image description function\n",
    "            image_description = get_image_description(image_src, content_before_image_str, content_after_image_str)\n",
    "            \n",
    "            # Update the image_description field in the item\n",
    "            item['image_description'] = image_description\n",
    "\n",
    "            if image_description:\n",
    "                log_message(f\"Write description for index {index+0} .\")\n",
    "            \n",
    "            # Every 100 items, write the data to the temporary file\n",
    "            if (index + 1) % batch_size == 0:\n",
    "                with open(temp_file_path, 'w', encoding='utf-8') as temp_file:\n",
    "                    json.dump(data, temp_file, ensure_ascii=False, indent=4)\n",
    "                log_message(f\"Batch write: Processed and wrote {index+1}/{len(data)} items.\")\n",
    "        except Exception as e:\n",
    "            log_message(f\"Error processing item {index+1} and src {image_src}. Error: {e}\")\n",
    "\n",
    "    # Write remaining items if total number is not a multiple of batch_size\n",
    "    if len(data) % batch_size != 0:\n",
    "        try:\n",
    "            with open(temp_file_path, 'w', encoding='utf-8') as temp_file:\n",
    "                json.dump(data, temp_file, ensure_ascii=False, indent=4)\n",
    "            log_message(f\"Final batch write: Processed and wrote all remaining items.\")\n",
    "        except Exception as e:\n",
    "            log_message(f\"Error writing final batch to temporary file: {temp_file_path}. Error: {e}\")\n",
    "\n",
    "    # Rename the temporary file to the original file after processing all items\n",
    "    try:\n",
    "        os.replace(temp_file_path, file_path)\n",
    "        log_message(f\"Successfully replaced original file with updated data: {file_path}\")\n",
    "    except Exception as e:\n",
    "        log_message(f\"Error replacing the original file with updated data: {file_path}. Error: {e}\")\n",
    "    \n",
    "    # Log the completion of the process\n",
    "    log_message(f\"=== Finished getting image description from file: {file_path} ===\")\n",
    "    \n",
    "\n",
    "\n",
    "# Extract text and images from the part with class=\"Mid2L_con\" and save to docs first, then JSON. n means how many lines of text was stored before and after each image.\n",
    "def extract_text_and_images(currenturl, tree, n=20):\n",
    "    \"\"\"\n",
    "    Extracts text and images from the part of the webpage with class=\"Mid2L_con\", \n",
    "    and splits into two parts: one with text only, and one with text + images.\n",
    "    Saves content to files first in 'docs', then processes and saves image metadata to JSON files.\n",
    "    \"\"\"\n",
    "    \n",
    "    if tree:\n",
    "\n",
    "        # Extract the page title\n",
    "        title = tree.xpath('//title/text()') # regular title\n",
    "        if title:\n",
    "            title_text = title[0].strip()\n",
    "        else:\n",
    "            title_text = 'No Title'\n",
    "    \n",
    "\n",
    "        # Extract the part of the page with class=\"Mid2L_con\"\n",
    "        mid2l_con = tree.xpath('//div[@class=\"Mid2L_con\"]')\n",
    "\n",
    "        # Clean the URL to make it filename-safe\n",
    "        filename_safe_url = currenturl.replace(\":\", \"=\").replace(\"/\", \"|\")\n",
    "\n",
    "        if mid2l_con:\n",
    "            text_content_list = [f\"Title: {title_text}\"]\n",
    "            text_with_images_list = [f\"Title: {title_text}\"]\n",
    "            txt_data_list = []\n",
    "            stop_extraction = False\n",
    "\n",
    "            # First pass: gather all text and image elements\n",
    "            for element in mid2l_con[0].iter():\n",
    "                if stop_extraction:\n",
    "                    break\n",
    "\n",
    "                # If it's a text node, extract the text and tail\n",
    "                if element.text and isinstance(element.tag, str):\n",
    "                    text = element.text.strip()\n",
    "                else:\n",
    "                    text = \"\"\n",
    "\n",
    "                # Also check the 'tail' for text outside the tag\n",
    "                if element.tail:\n",
    "                    tail_text = element.tail.strip()\n",
    "                else:\n",
    "                    tail_text = \"\"\n",
    "\n",
    "                # Combine text and tail_text\n",
    "                combined_text = text + \" \" + tail_text if text or tail_text else \"\"\n",
    "\n",
    "                # Append the combined text to the list if it's not empty and does not contain certain phrases\n",
    "                if combined_text:\n",
    "                    # Check if the combined text starts with \"本文由游民星空\"\n",
    "                    if combined_text.startswith((\"本文由游民星空\", \"推荐下载\")):\n",
    "                        stop_extraction = True\n",
    "                    else:\n",
    "                        # Check if combined_text contains any of the unwanted phrases\n",
    "                        unwanted_phrases = [\n",
    "                            \"更多相关内容请关注\",\n",
    "                            \"责任编辑\",\n",
    "                            \"友情提示：\",\n",
    "                            \"本文是否解决了您的问题\",\n",
    "                            \"已解决\",\n",
    "                            \"未解决\",\n",
    "                            \"黑神话：悟空专区\",\n",
    "                            \"上一页\",\n",
    "                            \"下一页\"\n",
    "                        ]\n",
    "                        if not any(phrase in combined_text for phrase in unwanted_phrases):\n",
    "                            text_content_list.append(combined_text)\n",
    "                            text_with_images_list.append(combined_text)\n",
    "\n",
    "\n",
    "                # If it's an <img> tag\n",
    "                if element.tag == 'img' and not stop_extraction:\n",
    "                    img_src = element.get('src')\n",
    "                    img_data_src = element.get('data-src', img_src)  # Use data-src if available, otherwise fallback to src\n",
    "                    img_alt = element.get('alt', '')\n",
    "                    img_title = element.get('title', '')\n",
    "                    img_width = element.get('width', '')\n",
    "                    img_height = element.get('height', '')\n",
    "\n",
    "                    # Convert relative paths to absolute URLs\n",
    "                    img_src = urljoin(currenturl, img_data_src)\n",
    "\n",
    "                    img_src=img_src.replace('_S.jpg', '.jpg')\n",
    "\n",
    "                    # Save the raw image to a file\n",
    "                    save_image_to_file(img_src)\n",
    "\n",
    "                    # Replace the placeholder with the actual image tag\n",
    "                    img_tag = f'<img src=\"{img_src}\" alt=\"{img_alt}\" width=\"{img_width}\" height=\"{img_height}\" title=\"{img_title}\">'\n",
    "                    text_with_images_list.append(img_tag)\n",
    "\n",
    "            # Convert text_content_list to a single string\n",
    "            text_content_str = '\\n'.join(text_content_list)\n",
    "            text_with_images_list_str = '\\n'.join(text_with_images_list)\n",
    "\n",
    "            # Save content to docs folder first\n",
    "            text_only_filename = os.path.join(\"docs/rawdata/\", f\"{filename_safe_url}_text_only.txt\")\n",
    "            text_with_images_filename = os.path.join(\"docs/rawdata/\", f\"{filename_safe_url}_text_with_images.html\")\n",
    "            save_content_to_file(text_content_str, text_only_filename)\n",
    "            save_content_to_file(text_with_images_list_str, text_with_images_filename)                \n",
    "\n",
    "            # Get img data\n",
    "            img_data_list = []\n",
    "            for img_index, line in enumerate(text_with_images_list):\n",
    "                if line.startswith(\"<img\"):\n",
    "                    # Extract image attributes\n",
    "                    img_src = line.split('src=\"')[1].split('\"')[0]\n",
    "                    img_alt = line.split('alt=\"')[1].split('\"')[0]\n",
    "                    img_width = line.split('width=\"')[1].split('\"')[0]\n",
    "                    img_height = line.split('height=\"')[1].split('\"')[0]\n",
    "                    img_title = line.split('title=\"')[1].split('\"')[0]\n",
    "                    \n",
    "                    # Get Base64 encoded image content (currently returning empty string)\n",
    "                    img_base64 = get_base64_encoded_image(img_src)\n",
    "                    \n",
    "                    # Get n lines before and after the image\n",
    "                    content_before_image = []\n",
    "                    content_after_image = []\n",
    "                    \n",
    "                    # Extract n lines before the image, stop if another <img> tag is encountered\n",
    "                    for i in range(img_index-1, max(0, img_index-n)-1, -1):\n",
    "                        # if '<img' in text_with_images_list[i]:\n",
    "                        #     break\n",
    "                        content_before_image.append(text_with_images_list[i])\n",
    "                    content_before_image.reverse()\n",
    "                    \n",
    "                    # Extract n lines after the image, stop if another <img> tag is encountered\n",
    "                    for i in range(img_index+1, min(len(text_with_images_list), img_index+1+n)):\n",
    "                        # if '<img' in text_with_images_list[i]:\n",
    "                        #     break\n",
    "                        content_after_image.append(text_with_images_list[i])\n",
    "                    \n",
    "                    content_before_image_str = '\\n'.join(content_before_image)\n",
    "                    content_after_image_str = '\\n'.join(content_after_image)\n",
    "                    image_descrip_str = ''\n",
    "                    \n",
    "                    # Add the image metadata to the img_data_list\n",
    "                    img_data_list.append({\n",
    "                        \"page_title\": title_text,  # To recognize this image with the page title.\n",
    "                        \"src\": img_src,\n",
    "                        \"base64\": img_base64,  # Temporarily set to an empty string\n",
    "                        \"title\": img_title,\n",
    "                        \"alt\": img_alt,\n",
    "                        \"content_before_image\": content_before_image_str,\n",
    "                        \"image_description\": image_descrip_str,\n",
    "                        \"content_after_image\": content_after_image_str,\n",
    "                        \"url\": currenturl,  # Current page url\n",
    "                        \"type\": \"img\"\n",
    "                    })\n",
    "\n",
    "\n",
    "            txt_data_list.append({\n",
    "                    \"txt\": text_content_str,\n",
    "                    \"url\": currenturl,\n",
    "                    \"type\": \"text\"\n",
    "                })\n",
    "\n",
    "            # Save the entire text_content_str directly to mmtext.json\n",
    "            save_data_json_with_format(txt_data_list, \"docs/mmtext.json\")\n",
    "\n",
    "            # Save the image metadata to JSON as a list of objects\n",
    "            save_data_json_with_format(img_data_list, \"docs/mmimg.json\")\n",
    "\n",
    "            return f\"Content saved to files in docs and JSON files processed.\"\n",
    "        else:\n",
    "            return \"No content found with class='Mid2L_con'.\"\n",
    "    else:\n",
    "        return \"Failed to fetch content.\"\n",
    "\n",
    "# Load existing links from the JSON file\n",
    "def load_existing_links(filename):\n",
    "    \"\"\"\n",
    "    Loads existing links from the specified JSON file.\n",
    "    If the file doesn't exist, it returns an empty list.\n",
    "    \"\"\"\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'r', encoding=\"utf-8\") as json_file:\n",
    "            return json.load(json_file)\n",
    "    return []\n",
    "    \n",
    "\n",
    "\n",
    "# Global variable to track new links added across function calls\n",
    "new_link_count = 0\n",
    "\n",
    "def save_link_to_json(new_link, filename=\"docs/links.json\"):\n",
    "    \"\"\"\n",
    "    Saves the provided link to the specified JSON file.\n",
    "    If the link contains .shtml?, it removes the string after .shtml.\n",
    "    If the link already exists, it returns False.\n",
    "    If the link is new and added, it returns True.\n",
    "    Logs the added link along with the updated total count of new links.\n",
    "    \"\"\"\n",
    "    global new_link_count  # Use the global counter for new links\n",
    "\n",
    "    # Check if the new_link contains '.shtml?'\n",
    "    if \".shtml?\" in new_link:\n",
    "        new_link = new_link.split(\".shtml?\")[0] + \".shtml\"\n",
    "    \n",
    "    links = load_existing_links(filename)\n",
    "    \n",
    "    if new_link not in links:\n",
    "        links.append(new_link)\n",
    "        new_link_count += 1  # Increment the global counter for a new link\n",
    "        with open(filename, 'w', encoding=\"utf-8\") as json_file:\n",
    "            json.dump(links, json_file, indent=4, ensure_ascii=False)\n",
    "        log_message(f\"New link added to links.json: {new_link}. Total links: {new_link_count}\")\n",
    "        return True\n",
    "    else:\n",
    "        log_message(f\"Link already in links.json: {new_link}\")\n",
    "        return False    \n",
    "\n",
    "\n",
    "\n",
    "# Global variable to track new URLs added across function calls\n",
    "new_url_count = 0\n",
    "\n",
    "# Save crawled URLs to the JSON file and count added URLs\n",
    "def save_crawled_url_to_json(new_url, filename=\"docs/crawled_urls.json\"):\n",
    "    \"\"\"\n",
    "    Saves the provided URL to the specified JSON file.\n",
    "    If the URL already exists, it returns False.\n",
    "    If the URL is new and added, it returns True.\n",
    "    Logs the added URL along with the updated total count of new URLs.\n",
    "    \"\"\"\n",
    "    global new_url_count  # Use the global counter for new URLs\n",
    "\n",
    "    # Check if the file exists and load existing URLs\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'r', encoding=\"utf-8\") as json_file:\n",
    "            urls = json.load(json_file)\n",
    "    else:\n",
    "        urls = [] \n",
    "    \n",
    "    log_message(f\"-----------------------------------------------------------\")\n",
    "    \n",
    "    # Check if the URL is new\n",
    "    if new_url not in urls:\n",
    "        urls.append(new_url)\n",
    "        new_url_count += 1  # Increment the global counter for a new URL\n",
    "        with open(filename, 'w', encoding=\"utf-8\") as json_file:\n",
    "            json.dump(urls, json_file, indent=4, ensure_ascii=False)\n",
    "        log_message(f\"New crawled url added to crawled_urls.json: {new_url}. Total urls: {new_url_count}\")\n",
    "        return True \n",
    "    else:\n",
    "        log_message(f\"Url already in crawled_urls.json: {new_url}\")\n",
    "        return False\n",
    "    \n",
    "\n",
    "# Check if the link exists in the JSON file\n",
    "def check_link_in_json(new_link, filename=\"docs/links.json\"):\n",
    "    \"\"\"\n",
    "    Checks if the provided link exists in the specified JSON file.\n",
    "    If the link contains .shtml?, it removes the string after .shtml.\n",
    "    Returns True if the link is found, otherwise False.\n",
    "    \"\"\"\n",
    "    # Check if the new_link contains '.shtml?'\n",
    "    if \".shtml?\" in new_link:\n",
    "        new_link = new_link.split(\".shtml?\")[0] + \".shtml\"\n",
    "    \n",
    "    # Load existing links from the JSON file\n",
    "    links = load_existing_links(filename)\n",
    "    \n",
    "    # Return True if the link is found, otherwise False\n",
    "    if new_link in links:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "# Send request with retry mechanism\n",
    "def fetch_url_with_retries(url, max_retries=2):\n",
    "    \"\"\"\n",
    "    Attempts to fetch content from the given URL, retrying up to max_retries times.\n",
    "    If the request fails, it waits for 1 second before retrying.\n",
    "    \"\"\"\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            response = requests.get(url, timeout=3)  # Set timeout to 3 seconds\n",
    "            \n",
    "            # If the status code is 200, the request was successful, return the content\n",
    "            if response.status_code == 200:\n",
    "                log_message(f\"Success on attempt {retries + 1} for {url}\")\n",
    "                return response.content\n",
    "            \n",
    "            # If the status code is not 200, log the failure reason\n",
    "            else:\n",
    "                log_message(f\"Attempt {retries + 1} failed with status code {response.status_code}\")\n",
    "        \n",
    "        except requests.RequestException as e:\n",
    "            # Capture request exceptions like timeout or connection errors\n",
    "            log_message(f\"Attempt {retries + 1} failed with error: {e}\")\n",
    "        \n",
    "        # Increment retry count\n",
    "        retries += 1\n",
    "        \n",
    "        # Wait for 1 second before retrying\n",
    "        time.sleep(1)\n",
    "\n",
    "    # If max retries are exceeded, return None or handle the error accordingly\n",
    "    log_message(f\"Failed to fetch the URL after {max_retries} attempts.\")\n",
    "    return None\n",
    "\n",
    "# New function to crawl the webpage and its linked pages up to a given depth\n",
    "def crawl_and_extract(url, keyword=\"黑神话\", linkdepth=2):\n",
    "    \"\"\"\n",
    "    Crawls the webpage starting from the given URL, and checks for links within the page.\n",
    "    If a page contains the term specified in 'keyword' in either \"Mid2L_con\" class or in the title,\n",
    "    or in the whole HTML content, it saves the link in 'links.json'.\n",
    "    Crawls up to the given linkdepth (including the original URL).\n",
    "    \"\"\"\n",
    "    def crawl_nest(url, current_depth, max_depth, pbar):\n",
    "\n",
    "        # Check if the link has been crawled\n",
    "        if save_crawled_url_to_json(url) == False:\n",
    "            return\n",
    "\n",
    "\n",
    "        # Check if the link exists in the JSON file and it is in the max depth, if yes, just return.\n",
    "        if check_link_in_json(url) == True and current_depth == max_depth:\n",
    "            log_message(f\"Link found in links.json: {url} (Depth: {current_depth})\")\n",
    "            return\n",
    "        \n",
    "        # Fetch the page content\n",
    "        html_content = fetch_url_with_retries(url)\n",
    "        if not html_content:\n",
    "            log_message(f\"Failed to fetch content for {url} (Depth: {current_depth})\")\n",
    "            return\n",
    "        \n",
    "        # Parse the HTML using lxml\n",
    "        tree = html.fromstring(html_content)\n",
    "\n",
    "        # Check if class=\"Mid2L_con\" or title contains the keyword\n",
    "        mid2l_con_elements = tree.xpath('//div[@class=\"Mid2L_con\"]')\n",
    "        title_elements = tree.xpath('//title/text()')\n",
    "        \n",
    "        # Check if keyword exists in Mid2L_con or Title\n",
    "        mid2l_con_text = mid2l_con_elements[0].text_content() if mid2l_con_elements else \"\"\n",
    "        title_text = title_elements[0] if title_elements else \"\"\n",
    "        \n",
    "        links = []\n",
    "\n",
    "        if mid2l_con_text:\n",
    "            if keyword in mid2l_con_text or keyword in title_text:\n",
    "                if current_depth == 0: current_depth = 1   # 0 is for url without mid2l_con, so we set it to 1\n",
    "                log_message(f\"Found '{keyword}' in Mid2L_con or Title at {url} (Depth: {current_depth})\")\n",
    "                linkexist = save_link_to_json(url)  # Save the link to JSON\n",
    "                if linkexist == True:\n",
    "                    extract_text_and_images(url, tree)\n",
    "                if current_depth < max_depth:\n",
    "                    # Get all links on the page\n",
    "                    alinks = tree.xpath('//div[@class=\"Mid2L_con\"]//a[@href]/@href')\n",
    "                    links = [urljoin(url, link) for link in alinks if link.startswith(('http', '/'))]\n",
    "                    links = list(set(links))\n",
    "                    # Remove unwanted link, links starting with 'javascript:', and those ending with '.jpg' or '.png'\n",
    "                    unwanted_link = \"\" # \"https://www.gamersky.com/z/bmwukong/\"\n",
    "                    filtered_links = [link for link in links if link != unwanted_link and not link.startswith('javascript:') and not link.endswith(('.jpg', '.png'))]\n",
    "                    links = filtered_links\n",
    "                    log_message(f\"Found {len(links)} links on {url} (Depth: {current_depth}). Crawling deeper...\")\n",
    "\n",
    "        else:\n",
    "            # If not found in Mid2L_con, check the full HTML content\n",
    "            if keyword in title_text: \n",
    "                current_depth = 0\n",
    "                log_message(f\"Found '{keyword}' in full HTML at {url} (Depth: {current_depth})\")\n",
    "            # if keyword in html_content.decode('utf-8', errors='ignore'):                \n",
    "                linkexist = save_link_to_json(url)  # Save the link to JSON\n",
    "                if linkexist == True:\n",
    "                    pass\n",
    "                    # extract_text_and_images(url, tree)   # Don't extract if it is just an overview\n",
    "\n",
    "                if current_depth < max_depth:\n",
    "                    # Get all links on the page\n",
    "                    alinks = tree.xpath('//a[@href]/@href')\n",
    "                    links = [urljoin(url, link) for link in alinks if link.startswith(('http', '/'))]\n",
    "                    links = list(set(links))\n",
    "                    # Remove unwanted link, links starting with 'javascript:', and those ending with '.jpg' or '.png'\n",
    "                    unwanted_link = \"\" # \"https://www.gamersky.com/z/bmwukong/\"\n",
    "                    filtered_links = [link for link in links if link != unwanted_link and not link.startswith('javascript:') and not link.endswith(('.jpg', '.png'))]\n",
    "                    links = filtered_links\n",
    "                    log_message(f\"Found {len(links)} links on {url} (Depth: {current_depth}). Crawling deeper...\")\n",
    "            else:\n",
    "                log_message(f\"No '{keyword}' found at {url} (Depth: {current_depth})\")\n",
    "\n",
    "        \n",
    "        if current_depth < max_depth and links:\n",
    "            current_depth = current_depth + 1\n",
    "            # Recursively crawl the found links, with increased depth\n",
    "            for link in tqdm(links, desc=f\"Crawling depth {current_depth}/{max_depth}\", leave=False, position=1, dynamic_ncols=True):\n",
    "                crawl_nest(link, current_depth, max_depth, pbar)\n",
    "                pbar.update(1)\n",
    "\n",
    "\n",
    "\n",
    "    # Write the start message\n",
    "    log_message(\"=== Crawl Start ===\")\n",
    "\n",
    "    with tqdm(total=100, desc=\"Crawling\", position=0, dynamic_ncols=True) as pbar:\n",
    "        crawl_nest(url, current_depth=0, max_depth=linkdepth, pbar=pbar)\n",
    "\n",
    "    # Write the end message with two empty lines\n",
    "    log_message(\"=== Crawl End ===\\n\\n\")\n",
    "\n",
    "# Test URL and Keyword\n",
    "url = \"https://www.gamersky.com/z/bmwukong/\"\n",
    "keyword = \"黑神话\"\n",
    "\n",
    "crawl_and_extract(url, keyword=keyword, linkdepth=2) # Crawl the URL and its linked pages up to a depth\n",
    "# get_all_image_description('docs/mmimg_nim.json') # Get image description for all images in the JSON file\n",
    "\n",
    "# image_src='http://img1.gamersky.com/image2024/08/20240819_qy_372_15/image077.jpg'\n",
    "# content_before_image_str =''\n",
    "# content_after_image_str=''\n",
    "# content_before_image_str='Title: 《黑神话悟空》珍玩图鉴 珍玩获取方法及效果一览\\n2024-08-20 10:19:28 来源：游民星空[原创] 作者：瑞破受气包  我要投稿\\n第13页：特品-金棕衣 \\n展开 \\n特品-金棕衣 \\n获取方法：【 \\n可能是焦面鬼王概率掉落 】。从第三回【极乐谷-长生大道】土地庙出发，进入土地庙前方木门之后往左前方走，击败前方雪地上的焦面鬼王（超大巨人）即可获得。具体路线请参考下文。\\n<img src=\\\"http://img1.gamersky.com/image2024/08/20240819_qy_372_15/image073.jpg\\\" alt=\\\"游民星空\\\" width=\\\"\\\" height=\\\"\\\" title=\\\"\\\">\\n<img src=\\\"http://img1.gamersky.com/image2024/08/20240819_qy_372_15/image075.jpg\\\" alt=\\\"游民星空\\\" width=\\\"\\\" height=\\\"\\\" title=\\\"\\\">\\n从第三回【极乐谷-长生大道】土地庙出发，进入土地庙前方木门之后往左前方走。 '\n",
    "# content_after_image_str='继续沿路前进，在拐弯处往右走，可以看到一大片雪地，还有一个超大巨人，巨人就是焦面鬼王，击杀即可获得。 \\n<img src=\\\"http://img1.gamersky.com/image2024/08/20240819_qy_372_15/image079.jpg\\\" alt=\\\"游民星空\\\" width=\\\"\\\" height=\\\"\\\" title=\\\"\\\">\\n11 \\n12 \\n13 \\n14 \\n15 \\n16 \\n17 \\n18 \\n19 \\n20 \\n21 \\n0 \\n0 \\n文章内容导航 \\n第1页：上品-猫睛宝串 \\n第2页：上品-玛瑙罐 \\n第3页：上品-不求人 \\n第4页：上品-砗磲佩 '\n",
    "\n",
    "# get_image_description(image_src, content_before_image_str, content_after_image_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized an empty vectorstore in vectorstore/chromadb-mmgamerag\n",
      "Quantity of existing_urls: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteProtocolError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/mmrag/lib/python3.11/site-packages/httpx/_transports/default.py:72\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mmrag/lib/python3.11/site-packages/httpx/_transports/default.py:236\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 236\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mmrag/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mmrag/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mmrag/lib/python3.11/site-packages/httpcore/_sync/connection.py:101\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mmrag/lib/python3.11/site-packages/httpcore/_sync/http11.py:143\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mmrag/lib/python3.11/site-packages/httpcore/_sync/http11.py:113\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    106\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    107\u001b[0m     (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m         trailing_data,\n\u001b[0;32m--> 113\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    115\u001b[0m         http_version,\n\u001b[1;32m    116\u001b[0m         status,\n\u001b[1;32m    117\u001b[0m         reason_phrase,\n\u001b[1;32m    118\u001b[0m         headers,\n\u001b[1;32m    119\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mmrag/lib/python3.11/site-packages/httpcore/_sync/http11.py:186\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mmrag/lib/python3.11/site-packages/httpcore/_sync/http11.py:238\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    237\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mServer disconnected without sending a response.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 238\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RemoteProtocolError(msg)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mreceive_data(data)\n",
      "\u001b[0;31mRemoteProtocolError\u001b[0m: Server disconnected without sending a response.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRemoteProtocolError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/mmrag/lib/python3.11/site-packages/openai/_base_client.py:991\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 991\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    993\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    994\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    995\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    996\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mmrag/lib/python3.11/site-packages/httpx/_client.py:926\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    924\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 926\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mmrag/lib/python3.11/site-packages/httpx/_client.py:954\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 954\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mmrag/lib/python3.11/site-packages/httpx/_client.py:991\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    989\u001b[0m     hook(request)\n\u001b[0;32m--> 991\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mmrag/lib/python3.11/site-packages/httpx/_client.py:1027\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1027\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mmrag/lib/python3.11/site-packages/httpx/_transports/default.py:235\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    223\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    224\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    225\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    233\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    234\u001b[0m )\n\u001b[0;32m--> 235\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_httpcore_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mmrag/lib/python3.11/contextlib.py:158\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen\u001b[38;5;241m.\u001b[39mthrow(typ, value, traceback)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mmrag/lib/python3.11/site-packages/httpx/_transports/default.py:89\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mRemoteProtocolError\u001b[0m: Server disconnected without sending a response.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 100\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;66;03m# Add documents and save to vectorstore\u001b[39;00m\n\u001b[1;32m     98\u001b[0m     add_img_documents_to_vectorstore(vectorstore, mmimgs)\n\u001b[0;32m--> 100\u001b[0m \u001b[43madd_txt_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Add texts and images to vectorstore\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 85\u001b[0m, in \u001b[0;36madd_txt_img\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m mmtexts \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     80\u001b[0m     Document(page_content\u001b[38;5;241m=\u001b[39mitem[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtxt\u001b[39m\u001b[38;5;124m'\u001b[39m], metadata\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m: item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m]})\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m txt_data_list\n\u001b[1;32m     82\u001b[0m ]\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Add documents and save to vectorstore\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m \u001b[43madd_text_documents_to_vectorstore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvectorstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmmtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Add imgs\u001b[39;00m\n\u001b[1;32m     89\u001b[0m mmimgs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     90\u001b[0m     Document(\n\u001b[1;32m     91\u001b[0m         page_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mimage_description:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_description\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m,  \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m img_data_list  \u001b[38;5;66;03m# Iterate over each item in img_data_list\u001b[39;00m\n\u001b[1;32m     95\u001b[0m ]\n",
      "Cell \u001b[0;32mIn[8], line 39\u001b[0m, in \u001b[0;36madd_text_documents_to_vectorstore\u001b[0;34m(vectorstore, documents)\u001b[0m\n\u001b[1;32m     36\u001b[0m new_documents \u001b[38;5;241m=\u001b[39m [doc \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents \u001b[38;5;28;01mif\u001b[39;00m doc\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m existing_urls]\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_documents:\n\u001b[0;32m---> 39\u001b[0m     \u001b[43mvectorstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# vectorstore.persist()  # Persist the vectorstore after adding documents\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(new_documents)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m new text documents.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mmrag/lib/python3.11/site-packages/langchain_core/vectorstores/base.py:287\u001b[0m, in \u001b[0;36mVectorStore.add_documents\u001b[0;34m(self, documents, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m     texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m    286\u001b[0m     metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m--> 287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`add_documents` and `add_texts` has not been implemented \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    291\u001b[0m )\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mmrag/lib/python3.11/site-packages/langchain_chroma/vectorstores.py:508\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[0;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    506\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 508\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadatas:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;66;03m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;66;03m# did not specify metadata for all texts\u001b[39;00m\n\u001b[1;32m    512\u001b[0m     length_diff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(texts) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(metadatas)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mmrag/lib/python3.11/site-packages/langchain_openai/embeddings/base.py:588\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[0;34m(self, texts, chunk_size)\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[1;32m    586\u001b[0m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[1;32m    587\u001b[0m engine \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeployment)\n\u001b[0;32m--> 588\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mmrag/lib/python3.11/site-packages/langchain_openai/embeddings/base.py:483\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[0;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[1;32m    481\u001b[0m batched_embeddings: List[List[\u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _iter:\n\u001b[0;32m--> 483\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_chunk_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invocation_params\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    487\u001b[0m         response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mmodel_dump()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mmrag/lib/python3.11/site-packages/openai/resources/embeddings.py:124\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[0;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    118\u001b[0m         embedding\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[1;32m    119\u001b[0m             base64\u001b[38;5;241m.\u001b[39mb64decode(data), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m         )\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mmrag/lib/python3.11/site-packages/openai/_base_client.py:1278\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1265\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1266\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1273\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1274\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1275\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1276\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1277\u001b[0m     )\n\u001b[0;32m-> 1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mmrag/lib/python3.11/site-packages/openai/_base_client.py:955\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    953\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 955\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mmrag/lib/python3.11/site-packages/openai/_base_client.py:1015\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1012\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered Exception\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1015\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1024\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaising connection error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(request\u001b[38;5;241m=\u001b[39mrequest) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mmrag/lib/python3.11/site-packages/openai/_base_client.py:1093\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1093\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mmrag/lib/python3.11/site-packages/openai/_base_client.py:1015\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1012\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered Exception\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1015\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1024\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaising connection error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(request\u001b[38;5;241m=\u001b[39mrequest) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mmrag/lib/python3.11/site-packages/openai/_base_client.py:1093\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1093\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mmrag/lib/python3.11/site-packages/openai/_base_client.py:981\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m    978\u001b[0m options \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_options(options)\n\u001b[1;32m    980\u001b[0m remaining_retries \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mget_max_retries(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries) \u001b[38;5;241m-\u001b[39m retries_taken\n\u001b[0;32m--> 981\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_request(request)\n\u001b[1;32m    984\u001b[0m kwargs: HttpxSendArgs \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mmrag/lib/python3.11/site-packages/openai/_base_client.py:506\u001b[0m, in \u001b[0;36mBaseClient._build_request\u001b[0;34m(self, options, retries_taken)\u001b[0m\n\u001b[1;32m    503\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextensions\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msni_hostname\u001b[39m\u001b[38;5;124m\"\u001b[39m: prepared_url\u001b[38;5;241m.\u001b[39mhost\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m)}\n\u001b[1;32m    505\u001b[0m \u001b[38;5;66;03m# TODO: report this error to httpx\u001b[39;00m\n\u001b[0;32m--> 506\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pyright: ignore[reportUnknownMemberType]\u001b[39;49;00m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNotGiven\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# the `Query` type that we use is incompatible with qs'\u001b[39;49;00m\n\u001b[1;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# `Params` type as it needs to be typed as `Mapping[str, object]`\u001b[39;49;00m\n\u001b[1;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# so that passing a `TypedDict` doesn't cause an error.\u001b[39;49;00m\n\u001b[1;32m    514\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# https://github.com/microsoft/pyright/issues/3526#event-6715453066\u001b[39;49;00m\n\u001b[1;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstringify\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMapping\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAny\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mmrag/lib/python3.11/site-packages/httpx/_client.py:358\u001b[0m, in \u001b[0;36mBaseClient.build_request\u001b[0;34m(self, method, url, content, data, files, json, params, headers, cookies, timeout, extensions)\u001b[0m\n\u001b[1;32m    352\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    353\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout\n\u001b[1;32m    354\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(timeout, UseClientDefault)\n\u001b[1;32m    355\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m Timeout(timeout)\n\u001b[1;32m    356\u001b[0m     )\n\u001b[1;32m    357\u001b[0m     extensions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextensions, timeout\u001b[38;5;241m=\u001b[39mtimeout\u001b[38;5;241m.\u001b[39mas_dict())\n\u001b[0;32m--> 358\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcookies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcookies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mmrag/lib/python3.11/site-packages/httpx/_models.py:342\u001b[0m, in \u001b[0;36mRequest.__init__\u001b[0;34m(self, method, url, params, headers, cookies, content, data, files, json, stream, extensions)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    341\u001b[0m     content_type: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent-type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 342\u001b[0m     headers, stream \u001b[38;5;241m=\u001b[39m \u001b[43mencode_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mboundary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_multipart_boundary_from_content_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontent_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcontent_type\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    351\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare(headers)\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m stream\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mmrag/lib/python3.11/site-packages/httpx/_content.py:214\u001b[0m, in \u001b[0;36mencode_request\u001b[0;34m(content, data, files, json, boundary)\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encode_urlencoded_data(data)\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m json \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mencode_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {}, ByteStream(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mmrag/lib/python3.11/site-packages/httpx/_content.py:177\u001b[0m, in \u001b[0;36mencode_json\u001b[0;34m(json)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode_json\u001b[39m(json: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m], ByteStream]:\n\u001b[0;32m--> 177\u001b[0m     body \u001b[38;5;241m=\u001b[39m \u001b[43mjson_dumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    178\u001b[0m     content_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(body))\n\u001b[1;32m    179\u001b[0m     content_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mmrag/lib/python3.11/json/__init__.py:231\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m# cached encoder\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m skipkeys \u001b[38;5;129;01mand\u001b[39;00m ensure_ascii \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     check_circular \u001b[38;5;129;01mand\u001b[39;00m allow_nan \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m separators \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     default \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sort_keys \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONEncoder\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mmrag/lib/python3.11/json/encoder.py:200\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m encode_basestring(o)\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_one_shot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m    202\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(chunks)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mmrag/lib/python3.11/json/encoder.py:258\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    254\u001b[0m     _iterencode \u001b[38;5;241m=\u001b[39m _make_iterencode(\n\u001b[1;32m    255\u001b[0m         markers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault, _encoder, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindent, floatstr,\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_keys,\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskipkeys, _one_shot)\n\u001b[0;32m--> 258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_iterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv,find_dotenv\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.schema.document import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from IPython.display import Markdown, display\n",
    "import os\n",
    "\n",
    "load_dotenv(find_dotenv()) \n",
    "\n",
    "# Preparation of documents for RAG-------------------------\n",
    "# Vectorstore, for retrieval\n",
    "embedding_model=OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "vectorstore_path = \"vectorstore/chromadb-mmgamerag\"\n",
    "if os.path.exists(vectorstore_path):\n",
    "    print(f\"Loaded vectorstore from disk: {vectorstore_path}\")\n",
    "else:\n",
    "    # Initialize an empty vectorstore and persist to disk\n",
    "    print(f\"Initialized an empty vectorstore in {vectorstore_path}\")\n",
    "\n",
    "vectorstore = Chroma(\n",
    "                embedding_function=embedding_model,\n",
    "                persist_directory=vectorstore_path,\n",
    "                ) \n",
    "\n",
    "def add_text_documents_to_vectorstore(vectorstore, documents):\n",
    "    # Retrieve existing documents from the vectorstore\n",
    "    existing_docs = vectorstore.get()\n",
    "    \n",
    "    existing_urls = [metadata['url'] for metadata in existing_docs['metadatas']] #???metadata\n",
    "    print(f\"Quantity of existing_urls: {len(existing_urls)}\")\n",
    "    # print(existing_urls)\n",
    "    # Filter out documents that already exist based on URL\n",
    "    new_documents = [doc for doc in documents if doc.metadata[\"url\"] not in existing_urls]\n",
    "\n",
    "    if new_documents:\n",
    "        vectorstore.add_documents(new_documents)\n",
    "        # vectorstore.persist()  # Persist the vectorstore after adding documents\n",
    "        print(f\"Added {len(new_documents)} new text documents.\")\n",
    "    else:\n",
    "        print(\"No new text documents to add.\")\n",
    "\n",
    "\n",
    "def add_img_documents_to_vectorstore(vectorstore, documents):\n",
    "    # Retrieve existing documents from the vectorstore\n",
    "    existing_docs = vectorstore.get()\n",
    "    \n",
    "    # Use `get` to avoid KeyError if some metadata does not have 'src'\n",
    "    existing_srcs = [metadata.get('src') for metadata in existing_docs['metadatas'] if 'src' in metadata]\n",
    "    print(f\"Quantity of existing_srcs: {len(existing_srcs)}\")\n",
    "    # print(existing_srcs)\n",
    "    \n",
    "    # Filter out documents that already exist based on src\n",
    "    new_documents = [doc for doc in documents if doc.metadata.get(\"src\") not in existing_srcs]\n",
    "\n",
    "    if new_documents:\n",
    "        vectorstore.add_documents(new_documents)\n",
    "        # vectorstore.persist()  # Persist the vectorstore after adding documents\n",
    "        print(f\"Added {len(new_documents)} new img documents.\")\n",
    "    else:\n",
    "        print(\"No new img documents to add.\")\n",
    "\n",
    "\n",
    "def add_txt_img():\n",
    "    txt_data_list = []\n",
    "    img_data_list = []\n",
    "\n",
    "    # Directly load and assign to txt_data_list from mmtext.json\n",
    "    with open('docs/mmtext.json', 'r', encoding='utf-8') as text_file:\n",
    "        txt_data_list = json.load(text_file)  # Assuming the JSON structure matches the required format\n",
    "\n",
    "    # Directly load and assign to img_data_list from mmimg.json\n",
    "    with open('docs/mmimg.json', 'r', encoding='utf-8') as img_file:\n",
    "        img_data_list = json.load(img_file)  # Assuming the JSON structure matches the required format\n",
    "\n",
    "    # Add texts\n",
    "    mmtexts = [\n",
    "        Document(page_content=item['txt'], metadata={\"url\": item['url'], \"type\": item['type']})\n",
    "        for item in txt_data_list\n",
    "    ]\n",
    "\n",
    "    # Add documents and save to vectorstore\n",
    "    add_text_documents_to_vectorstore(vectorstore, mmtexts)\n",
    "\n",
    "\n",
    "    # Add imgs\n",
    "    mmimgs = [\n",
    "        Document(\n",
    "            page_content=\"\\n\\nimage_description:\\n\" + item['image_description']  + '\\n',  \n",
    "            metadata={\"type\": item['type'], \"src\": item['src']}\n",
    "        )\n",
    "        for item in img_data_list  # Iterate over each item in img_data_list\n",
    "    ]\n",
    "\n",
    "    # Add documents and save to vectorstore\n",
    "    add_img_documents_to_vectorstore(vectorstore, mmimgs)\n",
    "\n",
    "add_txt_img() # Add texts and images to vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chrome vector store visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca2ae8b6e7f64a8ba1b6a9f116d1b409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import k3d\n",
    "from k3d.colormaps.matplotlib_color_maps import *\n",
    "\n",
    "# 获取所有嵌入向量\n",
    "embeddings = vectorstore.get(include=['embeddings'])['embeddings']\n",
    "docs = vectorstore.get(include=['embeddings'])['ids']\n",
    "\n",
    "\n",
    "# 限制数据量为前 1000 条，防止性能问题\n",
    "# embeddings = embeddings[:1000]\n",
    "# docs = docs[:1000]\n",
    "\n",
    "# 进行 PCA 降维到 3D\n",
    "pca = PCA(n_components=3)\n",
    "vis_dims = pca.fit_transform(embeddings)\n",
    "\n",
    "# 创建 K3D 点云数据\n",
    "points = np.array(vis_dims, dtype=np.float32)  # 转换为 float32 以兼容 K3D\n",
    "plot = k3d.plot()\n",
    "\n",
    "docs = [str(doc) for doc in docs]\n",
    "\n",
    "# 使用第一个 PCA 维度生成渐变色\n",
    "color_values = points[:, 0]  # 取第一个 PCA 维度值作为颜色映射基础\n",
    "scatter = k3d.points(\n",
    "    points,\n",
    "    point_size=0.006,  # 设置点大小\n",
    "    shader='mesh',  # 设置渲染器\n",
    "    color_map=Coolwarm,  # 使用 matplotlib 的 Inferno 渐变色映射\n",
    "    attribute=color_values,  # 使用 attribute 指定颜色属性\n",
    "    color_range=[color_values.min(), color_values.max()]  # 指定颜色范围\n",
    ")\n",
    "plot += scatter\n",
    "\n",
    "# 显示 3D 图形\n",
    "plot.display()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotly plot saved as '3d_plot_plotly.html'\n"
     ]
    }
   ],
   "source": [
    "# 导入必要库\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "\n",
    "# 获取所有嵌入向量和文档 ID\n",
    "embeddings = vectorstore.get(include=['embeddings'])['embeddings']\n",
    "docs = vectorstore.get(include=['embeddings'])['ids']\n",
    "docs_content = vectorstore.get(include=['documents'])['documents']\n",
    "docs_id = [doc_id for doc_id in docs]\n",
    "docs_content_short = [doc[:60] for doc in docs_content]\n",
    "text_data = [\n",
    "    f\"{doc_id}<br>{'[PAGE]' if doc.startswith('Title') else '[IMG]'}{doc}\"\n",
    "    for doc_id, doc in zip(docs_id, docs_content_short)\n",
    "]\n",
    "\n",
    "# 限制数据量为前 1000 条，防止性能问题\n",
    "# embeddings = embeddings[:1000]\n",
    "# docs = docs[:1000]\n",
    "\n",
    "# 进行 PCA 降维到 3D\n",
    "pca = PCA(n_components=3)\n",
    "vis_dims = pca.fit_transform(embeddings)\n",
    "\n",
    "# 提取 PCA 降维后的坐标\n",
    "x = vis_dims[:, 0]\n",
    "y = vis_dims[:, 1]\n",
    "z = vis_dims[:, 2]\n",
    "\n",
    "# 使用第一个 PCA 维度作为颜色映射基础\n",
    "color = vis_dims[:, 0]\n",
    "point_colors = [\n",
    "    \"coral\" if doc.startswith(\"Title\") else \"skyblue\" for doc in docs_content\n",
    "]\n",
    "\n",
    "# 绘制交互式 3D 散点图\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=x, \n",
    "    y=y, \n",
    "    z=z, \n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=[5] * len(docs_id),\n",
    "        color=point_colors,  # 颜色对应到 color 数据\n",
    "        colorscale=[[0, \"gray\"], [1, \"#4f4f4f\"]],  # 设置颜色渐变从灰到深灰\n",
    "        opacity=1\n",
    "    ),\n",
    "\n",
    "    text=text_data,  # 绑定文档信息\n",
    "    ids=docs_id,\n",
    "    hovertemplate=(\n",
    "        'PCA1: %{x}<br>' +\n",
    "        'PCA2: %{y}<br>' +\n",
    "        'PCA3: %{z}<br>' +\n",
    "        'Doc_id: %{text}<extra></extra>'\n",
    "    )\n",
    "))\n",
    "\n",
    "# 设置布局\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis_title='PCA1',\n",
    "        yaxis_title='PCA2',\n",
    "        zaxis_title='PCA3'\n",
    "    )\n",
    ")\n",
    "\n",
    "# 保存为 HTML 文件\n",
    "pio.write_html(fig, file=\"3d_plot_plotly.html\", auto_open=True)\n",
    "\n",
    "print(\"Plotly plot saved as '3d_plot_plotly.html'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotly plot saved as '3d_plot_plotly.html'\n"
     ]
    }
   ],
   "source": [
    "# 假设新的颜色为 \"purple\"，针对索引 0 到 1000 的点\n",
    "new_color_range = range(0, 1000)\n",
    "new_color_value = \"purple\"\n",
    "\n",
    "# 确保 marker.color 是一个可修改的列表\n",
    "current_colors = list(fig.data[0].marker.color)\n",
    "\n",
    "# 更新索引 0 至 1000 的颜色\n",
    "for idx in new_color_range:\n",
    "    if idx < len(current_colors):  # 防止索引越界\n",
    "        current_colors[idx] = new_color_value\n",
    "\n",
    "# 应用更新后的颜色\n",
    "fig.data[0].marker.color = current_colors\n",
    "\n",
    "# 保存为 HTML 文件\n",
    "pio.write_html(fig, file=\"3d_plot_plotly.html\", auto_open=True)\n",
    "\n",
    "print(\"Plotly plot saved as '3d_plot_plotly.html'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotly plot updated and saved as '3d_plot_plotly_updated.html'\n"
     ]
    }
   ],
   "source": [
    "# target_ids = [\"d81a07e6-69e1-40e9-873a-41d7ad4a5f12\", \"e510e2d4-62ae-403c-8947-081708660783\", \"c594d133-4e3f-4084-a501-b16beaf9d664\"]\n",
    "# id_indices = {doc_id: docs.index(doc_id) for doc_id in target_ids if doc_id in docs}\n",
    "\n",
    "# 定义需要匹配的特定关键字符\n",
    "target_content_keywords = [\"Title: 《黑神话悟空》全BOSS打法指南 全BOSS位置与招式解析\", \"specific_word2\", \"specific_word3\"]\n",
    "\n",
    "# 找出满足条件的 doc_id 的索引\n",
    "id_indices = {\n",
    "    docs_id[idx]: idx\n",
    "    for idx, content in enumerate(docs_content)\n",
    "    if any(keyword in content for keyword in target_content_keywords)\n",
    "}\n",
    "\n",
    "# 确保 marker.color 和 marker.size 是列表\n",
    "current_colors = list(fig.data[0].marker.color)\n",
    "current_sizes = list(fig.data[0].marker.size)\n",
    "\n",
    "# 遍历所有 doc_id，并根据 id_indices 更新点的颜色和大小\n",
    "for idx, doc_id in enumerate(docs_id):\n",
    "    if doc_id in id_indices:\n",
    "        # 更新颜色：如果以 \"Title\" 开头，设置为 red，否则为 blue\n",
    "        current_colors[idx] = \"red\" if docs_content[idx].startswith(\"Title\") else \"blue\"\n",
    "        # 更新大小\n",
    "        current_sizes[idx] = new_size\n",
    "\n",
    "# 更新图表中的颜色和大小\n",
    "fig.data[0].marker.color = current_colors\n",
    "fig.data[0].marker.size = current_sizes\n",
    "\n",
    "# 保存更新后的图表\n",
    "pio.write_html(fig, file=\"3d_plot_plotly_updated.html\", auto_open=True)\n",
    "\n",
    "print(\"Plotly plot updated and saved as '3d_plot_plotly_updated.html'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]:  URL 'https://cloud.r-project.org/bin/macosx/big-sur-arm64/contrib/4.3/threejs_0.3.3.tgz' を試しています \n",
      "\n",
      "R[write to console]: Content type 'application/x-gzip'\n",
      "R[write to console]:  length 942740 bytes (920 KB)\n",
      "\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: \n",
      "\n",
      "R[write to console]: downloaded 920 KB\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ダウンロードされたパッケージは、以下にあります \n",
      " \t/var/folders/03/4yvvy5rx6c97ntf957y4f7fw0000gn/T//Rtmp64CAEh/downloaded_packages \n"
     ]
    }
   ],
   "source": [
    "# 安装 threejs 包\n",
    "import rpy2.robjects.packages as rpackages\n",
    "utils = rpackages.importr('utils')\n",
    "\n",
    "# 设置安装 CRAN 镜像\n",
    "utils.chooseCRANmirror(ind=1)  # 选择第一个可用的镜像\n",
    "\n",
    "# 安装 threejs 包\n",
    "if not rpackages.isinstalled('threejs'):\n",
    "    utils.install_packages('threejs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "\n",
    "# retrieved_docs = retriever.invoke(\"猫睛宝串\")\n",
    "# retrieved_docs\n",
    "\n",
    "retrieved_docs = vectorstore.similarity_search_with_relevance_scores(query=\"君子牌\", k=5, filter={\"type\": \"text\"})\n",
    "\n",
    "# Iterate over retrieved_docs and extract the url, page_content, and score\n",
    "for doc, score in retrieved_docs:\n",
    "    url = doc.metadata.get('url', 'No URL found')  # Extract the URL from the metadata\n",
    "    type = doc.metadata.get('type', '') \n",
    "    page_content = doc.page_content  # Get the page content\n",
    "    # print(f\"URL: {url}\\nContent: {page_content}\\nScore: {score}\\nType: {type}\\n\")\n",
    "\n",
    "retrieved_docs = vectorstore.similarity_search_with_relevance_scores(query=\"君子牌\", k=5, filter={\"type\": \"img\"})\n",
    "\n",
    "# Iterate over retrieved_docs and extract the url, page_content, and score\n",
    "for doc, score in retrieved_docs:\n",
    "    url = doc.metadata.get('url', 'No URL found')  # Extract the URL from the metadata\n",
    "    type = doc.metadata.get('type', '') \n",
    "    src = doc.metadata.get('src', '') \n",
    "    page_content = doc.page_content  # Get the page content\n",
    "    print(f\"URL: {url}\\nSRC: {src}\\nContent: {page_content}\\nScore: {score}\\nType: {type}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Q&A with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display, Image\n",
    "\n",
    "mmgamellm = ChatOpenAI(name=\"MMGameRag\", model_name=\"gpt-4o-mini\", temperature=0.6, streaming=True)\n",
    "\n",
    "def format_docs(docs_with_scores):\n",
    "    \"\"\"\n",
    "    Formats the retrieved documents into a string with their content, URL, and score,\n",
    "    and lists them in order with numbering.\n",
    "    \"\"\"\n",
    "    formatted_docs = []\n",
    "    \n",
    "    # Iterate over the documents and their associated scores\n",
    "    for i, (doc, score) in enumerate(docs_with_scores, 1):  # Enumerate to add numbering starting from 1\n",
    "        imgsrc = doc.metadata.get('src', '')\n",
    "        if imgsrc: # Image\n",
    "            formatted_doc = (\n",
    "                f\"{i}.\\n\"\n",
    "                f\"Image Content:\\n{doc.page_content}\\n\"  # Content of the document\n",
    "                f\"Page Url: {doc.metadata.get('url', '')}\\n\"  # Assuming URL is stored in metadata\n",
    "                f\"Image Src: {doc.metadata.get('src', '')}\\n\"  # Assuming URL is stored in metadata\n",
    "                f\"Score: {score}\\n\"  # Similarity score for the document\n",
    "            )\n",
    "        else:  # Text\n",
    "            formatted_doc = (\n",
    "                f\"{i}.\\n\"\n",
    "                f\"Text Content:\\n{doc.page_content}\\n\"  # Content of the document\n",
    "                f\"Page Url: {doc.metadata.get('url', '')}\\n\"  # Assuming URL is stored in metadata\n",
    "                f\"Score: {score}\\n\"  # Similarity score for the document\n",
    "            )\n",
    "        formatted_docs.append(formatted_doc)  # Add formatted document to the list\n",
    "    \n",
    "    return \"\\n\".join(formatted_docs)  # Join all formatted documents into a single string\n",
    "\n",
    "# Prompt for code generation\n",
    "prompt_template = \"\"\"你是《黑神话：悟空》这款游戏的AI助手，根据Question和Context专门为玩家提供详尽的游戏攻略并以Markdown的格式输出.请注意：\n",
    "1. 在Image中找到与Question和Answer最相关的图像。每个Image都有Text before image，Image descriptioin和Text after image，可以用来判断这个Image应该被插入到与文本答案最匹配的上下文的哪个段落当中。格式如下：\n",
    "    \n",
    "    文本答案段落\n",
    "    [![](图像1的Src)](图像1的Url)\n",
    "    文本答案段落\n",
    "    [![](图像2的Src)](图像2的Url)\n",
    "    文本答案段落\n",
    "    ...\n",
    "\n",
    "2. 在输出答案的最后，根据问题找到context中的最相关的几个参考文档，并列出Url链接，以供用户参考原始文档。\n",
    "\n",
    "Question: \n",
    "{question}\n",
    "\n",
    "Context: \n",
    "{context}\n",
    "\n",
    "Image:\n",
    "{image}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt_code = ChatPromptTemplate.from_template(prompt_template)\n",
    "\n",
    "chain = (\n",
    "    prompt_code\n",
    "    | mmgamellm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "gamer_question = \"黑神话一共有多少上品珍宝？举几个例子\"\n",
    "context_retrieval = format_docs(vectorstore.similarity_search_with_score(query=gamer_question, k=5, filter={\"type\": \"text\"}))\n",
    "# print(context_retrieval + \"\\n------------------------\\n\")\n",
    "img_retrieval = format_docs(vectorstore.similarity_search_with_score(query=gamer_question, k=5, filter={\"type\": \"img\"}))\n",
    "# print(img_retrieval + \"\\n------------------------\\n\")\n",
    "result = chain.invoke({\n",
    "    \"question\": gamer_question, \n",
    "    \"context\": context_retrieval,\n",
    "    \"image\": img_retrieval\n",
    "})\n",
    "\n",
    "\n",
    "display(Markdown(result))\n",
    "# display(Image(url=\"http://img1.gamersky.com/image2024/08/20240819_qy_372_15/image001_S.jpg\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temp --  Get page title for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items: 100%|██████████| 19753/19753 [46:39<00:00,  7.05it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titles have been updated and page_title is now the first attribute in the JSON file.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "from lxml import html\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to clean and format the title text\n",
    "def clean_title_text(title_text):\n",
    "    \"\"\"\n",
    "    Clean and format the extracted title text by removing empty lines and extra spaces.\n",
    "    \"\"\"\n",
    "    lines = [line.strip() for line in title_text.splitlines() if line.strip()]\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# Read the first 10 items from the JSON file\n",
    "with open('docs/mmimg.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Loop through the first 10 items with tqdm progress bar\n",
    "# for i, item in tqdm(enumerate(data[:10]), total=10, desc=\"Processing items\"):\n",
    "for i, item in tqdm(enumerate(data), total=len(data), desc=\"Processing items\"):\n",
    "    url = item.get('url', '')  # Get the URL from the item\n",
    "    if not url:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # Fetch the HTML content from the URL\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check for HTTP errors\n",
    "        html_content = response.content\n",
    "        \n",
    "        # Parse the HTML using lxml\n",
    "        tree = html.fromstring(html_content)\n",
    "\n",
    "        title_text = ''\n",
    "        # Extract title from div with class \"Mid2L_tit\"\n",
    "        mid2L_tit_elements = tree.xpath('//div[@class=\"Mid2L_tit\"]')\n",
    "        if mid2L_tit_elements:\n",
    "            title_text = clean_title_text(mid2L_tit_elements[0].text_content())\n",
    "        else:\n",
    "            # If no title in Mid2L_tit, use the page's main title\n",
    "            title_elements = tree.xpath('//title')\n",
    "            if title_elements:\n",
    "                title_text = clean_title_text(title_elements[0].text_content())\n",
    "\n",
    "        # Reorder dictionary to have page_title as the first attribute\n",
    "        # First, remove page_title if it exists\n",
    "        if 'page_title' in item:\n",
    "            del item['page_title']\n",
    "        \n",
    "        # Create a new ordered dict with page_title as the first item\n",
    "        ordered_item = OrderedDict([('page_title', title_text)] + list(item.items()))\n",
    "\n",
    "        # Replace the old item with the newly ordered item\n",
    "        data[i] = ordered_item\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {url}: {e}\")\n",
    "\n",
    "# Save the modified JSON data back to the file\n",
    "with open('docs/mmimg.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Titles have been updated and page_title is now the first attribute in the JSON file.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get titles \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items: 100%|██████████| 5430/5430 [00:00<00:00, 279479.58it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define function to extract and write titles with progress display\n",
    "def extract_titles():\n",
    "    \"\"\"\n",
    "    Reads each item's 'txt' in docs/mmtext.json, processes the titles based on URL conditions,\n",
    "    and writes the result to docs/titles.json with a progress bar.\n",
    "    \"\"\"\n",
    "    input_path = \"docs/mmtext.json\"\n",
    "    output_path = \"docs/titles.json\"\n",
    "    titles = []\n",
    "\n",
    "    # Read input JSON file\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        items = json.load(f)\n",
    "    \n",
    "    # Process each item with tqdm progress bar\n",
    "    for item in tqdm(items, desc=\"Processing items\"):\n",
    "        url = item.get(\"url\", \"\")\n",
    "        lines = item.get(\"txt\", \"\").splitlines()\n",
    "        title_data = {}\n",
    "\n",
    "        if \"www.gamersky.com/handbook\" in url:\n",
    "            # Remove specific prefix and suffix\n",
    "            title_str = lines[0].replace(\"Title: 《黑神话悟空》\", \"\").replace(\"-游民星空 GamerSky.com\", \"\")\n",
    "            # Split by underscore\n",
    "            if \"_\" in title_str:\n",
    "                title, subtitle = title_str.split(\"_\", 1)\n",
    "            else:\n",
    "                title, subtitle = title_str, \"\"\n",
    "            title_data = {\"title\": title, \"subtitle\": subtitle, \"class\": \"攻略\"}\n",
    "\n",
    "        elif \"www.gamersky.com/news\" in url:\n",
    "            # Remove specific prefix and suffix\n",
    "            title = lines[0].replace(\"Title: \", \"\").replace(\" _ 游民星空 GamerSky.com\", \"\")\n",
    "            title_data = {\"title\": title, \"class\": \"新闻\"}\n",
    "\n",
    "        elif \"down.gamersky.com/\" in url:\n",
    "            # Remove specific prefix and suffix\n",
    "            title = lines[0].replace(\"Title: \", \"\").replace(\"_黑神话：悟空下载 - 游民星空下载中心\", \"\")\n",
    "            title_data = {\"title\": title, \"class\": \"下载\"}\n",
    "\n",
    "        if title_data:\n",
    "            titles.append(title_data)\n",
    "    \n",
    "    # Write output JSON file\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(titles, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Execute the function\n",
    "extract_titles()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basic rag- only keep image discrip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed file has been saved as 'docs/mmimg.json'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON file\n",
    "with open('docs/mmimg_all.json', 'r', encoding='utf-8') as file:\n",
    "    # Parse the JSON content into a Python object\n",
    "    data = json.load(file)\n",
    "\n",
    "# Create a new list with only 'src' and 'image_description' for each item\n",
    "processed_data = []\n",
    "for item in data:\n",
    "    # Extract 'src' and 'image_description', set default values if missing\n",
    "    processed_item = {\n",
    "        'src': item.get('src', ''),\n",
    "        'image_description': item.get('image_description', ''),\n",
    "        'type': item.get('type', '')\n",
    "    }\n",
    "    processed_data.append(processed_item)\n",
    "\n",
    "# Save the processed data to a new JSON file\n",
    "with open('docs/mmimg.json', 'w', encoding='utf-8') as file:\n",
    "    # Write the data back as JSON with indentation for readability\n",
    "    json.dump(processed_data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Processed file has been saved as 'docs/mmimg.json'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
