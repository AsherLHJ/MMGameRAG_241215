{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with RecursiveUrlLoader (discarded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_community.document_loaders import RecursiveUrlLoader\n",
    "\n",
    "# Extracts text content from HTML, removing extra newlines and formatting it for readability.\n",
    "def bs4_extractor(html: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts clean text from the given HTML content, removing extra newlines for better readability.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n",
    "\n",
    "# Extracts the HTML content, preserving text and <img> tags, and placing the image in the text where found.\n",
    "def extract_text_with_images(html: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts both text and <img> tags from the given HTML content, placing images within the text where found.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    \n",
    "    output = \"\"\n",
    "    for element in soup.descendants:\n",
    "        if element.name not in ['script', 'style'] and isinstance(element, str):\n",
    "            output += element.strip() + \"\\n\\n\"\n",
    "        elif element.name == \"img\":\n",
    "            img_tag = f'<img src=\"{element.get(\"src\")}\", data-src=\"{element.get(\"data-src\")}\", alt=\"{element.get(\"alt\")}\", width=\"{element.get(\"width\")}\", height=\"{element.get(\"height\")}\">\\n\\n'\n",
    "            output += img_tag\n",
    "    \n",
    "    return re.sub(r\"\\n\\n+\", \"\\n\", output).strip()\n",
    "\n",
    "# Saves the given content to a file with the specified filename.\n",
    "def save_to_file(content: str, filename: str) -> None:\n",
    "    \"\"\"\n",
    "    Saves the content to a file with the given filename.\n",
    "    \"\"\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(content)\n",
    "\n",
    "# Process the loaded content and save the results\n",
    "def process_html_content(html_content: str, doc_i) -> None:\n",
    "    \"\"\"\n",
    "    Processes the HTML content by extracting text and images, and saves the results into files.\n",
    "    It also extracts the title and uses it to name the saved files.\n",
    "    \"\"\"\n",
    "    # Use BeautifulSoup to extract the title\n",
    "    soup = BeautifulSoup(html_content, \"lxml\")\n",
    "    title = soup.title.string if soup.title else \"untitled\"\n",
    "    \n",
    "    # Clean title for valid filename usage\n",
    "    safe_title = re.sub(r'[\\/:*?\"<>|]', \"_\", title)\n",
    "    \n",
    "    # Extract text content\n",
    "    extracted_text = bs4_extractor(html_content)\n",
    "    \n",
    "    # Extract text with images\n",
    "    text_with_images = extract_text_with_images(html_content)\n",
    "    \n",
    "    # Save files using the title as part of the filename\n",
    "    save_to_file(html_content, f\"{safe_title}[{doc_i}]_original_html.txt\")\n",
    "    save_to_file(extracted_text, f\"{safe_title}[{doc_i}]_text.txt\")\n",
    "    save_to_file(text_with_images, f\"{safe_title}[{doc_i}]_text_with_images.txt\")\n",
    "    \n",
    "    print(f\"Files saved with the title '{safe_title}'\")\n",
    "\n",
    "# Define the URL to be processed\n",
    "currenturl = \"https://www.gamersky.com/z/bmwukong/1314156_195585/\"\n",
    "\n",
    "# Use RecursiveUrlLoader without an extractor to get the original HTML content\n",
    "loader_html = RecursiveUrlLoader(\n",
    "    currenturl,\n",
    "    max_depth=10,\n",
    "    use_async=False,\n",
    "    extractor=None,  # No extractor here to get the original HTML\n",
    "    metadata_extractor=None,\n",
    "    exclude_dirs=(),\n",
    "    timeout=10,\n",
    "    check_response_status=True,\n",
    "    continue_on_failure=True,\n",
    "    prevent_outside=True,\n",
    "    base_url=None,\n",
    ")\n",
    "\n",
    "# Load the original HTML content\n",
    "docs_html = loader_html.load()\n",
    "\n",
    "# Process all documents loaded from RecursiveUrlLoader\n",
    "if docs_html and len(docs_html) > 0:\n",
    "    for i, doc in enumerate(docs_html):\n",
    "        html_content = doc.page_content\n",
    "        \n",
    "        # Optional: Include document index as part of the filename to differentiate files\n",
    "        print(f\"Processing document {i + 1}/{len(docs_html)}\")\n",
    "\n",
    "        # Process the HTML content by extracting text and images for each document\n",
    "        process_html_content(html_content, i)\n",
    "else:\n",
    "    print(\"Failed to load any content from the URL.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "\n",
    "display(Markdown(html_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import RecursiveUrlLoader\n",
    "\n",
    "# 定义要抓取的初始 URL\n",
    "currenturl = \"https://www.gamersky.com/z/bmwukong/\"\n",
    "\n",
    "# 配置 RecursiveUrlLoader\n",
    "loader = RecursiveUrlLoader(\n",
    "    currenturl,\n",
    "    max_depth=2,  # 设置递归抓取深度，例如 3 表示抓取当前页面及其链接的两级页面\n",
    "    use_async=False,  # 是否异步抓取\n",
    "    extractor=None,  # 提取器设为 None 以获取原始 HTML\n",
    "    metadata_extractor=None,  # 不使用元数据提取器\n",
    "    exclude_dirs=(),  # 可选，排除不需要抓取的目录\n",
    "    timeout=5,  # 每个页面的抓取超时时间\n",
    "    check_response_status=True,  # 是否检查 HTTP 响应状态码\n",
    "    continue_on_failure=True,  # 是否在遇到错误时继续抓取\n",
    "    prevent_outside=False,  # 防止抓取超出指定 URL 域名或目录的链接\n",
    "    base_url=currenturl,  # 确保只抓取从这个 URL 开始的页面\n",
    ")\n",
    "\n",
    "# 加载文档，返回一个包含所有递归抓取到页面的列表\n",
    "docs = loader.load()\n",
    "\n",
    "# 处理抓取到的文档\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"\\n-----------------\")\n",
    "    print(f\"Document {i+1}:\")  \n",
    "    print(doc.page_content.__len__())  # 输出每个文档的内容\n",
    "    print(doc.metadata)  # 输出每个文档的内容\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Complete page scrwling and data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from lxml import html, etree\n",
    "from urllib.parse import urljoin\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm  # Progress bar\n",
    "import json\n",
    "import urllib.request\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI \n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from io import BytesIO\n",
    "import base64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling:   1%|          | 1/100 [00:00<00:42,  2.34it/s]/var/folders/03/4yvvy5rx6c97ntf957y4f7fw0000gn/T/ipykernel_14172/3079815839.py:187: FutureWarning: The behavior of this method will change in future versions. Use specific 'len(elem)' or 'elem is not None' test instead.\n",
      "  if tree:\n",
      "Crawling: 506it [04:21,  1.93it/s]                         \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 578\u001b[0m\n\u001b[1;32m    575\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.gamersky.com/z/bmwukong/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m keyword \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m黑神话\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 578\u001b[0m \u001b[43mcrawl_and_extract\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeyword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeyword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinkdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Crawl the URL and its linked pages up to a depth\u001b[39;00m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;66;03m# get_all_image_description('docs/mmimg.json') # Get image description for all images in the JSON file\u001b[39;00m\n\u001b[1;32m    580\u001b[0m \n\u001b[1;32m    581\u001b[0m \u001b[38;5;66;03m# image_src='http://img1.gamersky.com/image2024/08/20240819_qy_372_15/image077.jpg'\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    586\u001b[0m \n\u001b[1;32m    587\u001b[0m \u001b[38;5;66;03m# get_image_description(image_src, content_before_image_str, content_after_image_str)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[29], line 569\u001b[0m, in \u001b[0;36mcrawl_and_extract\u001b[0;34m(url, keyword, linkdepth)\u001b[0m\n\u001b[1;32m    566\u001b[0m log_message(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== Crawl Start ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCrawling\u001b[39m\u001b[38;5;124m\"\u001b[39m, position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, dynamic_ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m--> 569\u001b[0m     \u001b[43mcrawl\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlinkdepth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpbar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;66;03m# Write the end message with two empty lines\u001b[39;00m\n\u001b[1;32m    572\u001b[0m log_message(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== Crawl End ===\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[29], line 560\u001b[0m, in \u001b[0;36mcrawl_and_extract.<locals>.crawl\u001b[0;34m(url, current_depth, max_depth, pbar)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;66;03m# Recursively crawl the found links, with increased depth\u001b[39;00m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m link \u001b[38;5;129;01min\u001b[39;00m tqdm(links, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCrawling depth \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_depth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_depth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, dynamic_ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 560\u001b[0m     \u001b[43mcrawl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlink\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[29], line 560\u001b[0m, in \u001b[0;36mcrawl_and_extract.<locals>.crawl\u001b[0;34m(url, current_depth, max_depth, pbar)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;66;03m# Recursively crawl the found links, with increased depth\u001b[39;00m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m link \u001b[38;5;129;01min\u001b[39;00m tqdm(links, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCrawling depth \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_depth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_depth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, dynamic_ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 560\u001b[0m     \u001b[43mcrawl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlink\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n",
      "    \u001b[0;31m[... skipping similar frames: crawl_and_extract.<locals>.crawl at line 560 (5 times)]\u001b[0m\n",
      "Cell \u001b[0;32mIn[29], line 560\u001b[0m, in \u001b[0;36mcrawl_and_extract.<locals>.crawl\u001b[0;34m(url, current_depth, max_depth, pbar)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;66;03m# Recursively crawl the found links, with increased depth\u001b[39;00m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m link \u001b[38;5;129;01min\u001b[39;00m tqdm(links, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCrawling depth \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_depth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_depth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, dynamic_ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 560\u001b[0m     \u001b[43mcrawl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlink\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[29], line 519\u001b[0m, in \u001b[0;36mcrawl_and_extract.<locals>.crawl\u001b[0;34m(url, current_depth, max_depth, pbar)\u001b[0m\n\u001b[1;32m    517\u001b[0m linkexist \u001b[38;5;241m=\u001b[39m save_link_to_json(url)  \u001b[38;5;66;03m# Save the link to JSON\u001b[39;00m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m linkexist \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 519\u001b[0m     \u001b[43mextract_text_and_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtree\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_depth \u001b[38;5;241m<\u001b[39m max_depth:\n\u001b[1;32m    521\u001b[0m     \u001b[38;5;66;03m# Get all links on the page\u001b[39;00m\n\u001b[1;32m    522\u001b[0m     alinks \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mxpath(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m//div[@class=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMid2L_con\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]//a[@href]/@href\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[29], line 275\u001b[0m, in \u001b[0;36mextract_text_and_images\u001b[0;34m(currenturl, tree, n)\u001b[0m\n\u001b[1;32m    272\u001b[0m img_src\u001b[38;5;241m=\u001b[39mimg_src\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_S.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    274\u001b[0m \u001b[38;5;66;03m# Save the raw image to a file\u001b[39;00m\n\u001b[0;32m--> 275\u001b[0m \u001b[43msave_image_to_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_src\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;66;03m# Replace the placeholder with the actual image tag\u001b[39;00m\n\u001b[1;32m    278\u001b[0m img_tag \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<img src=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_src\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m alt=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_alt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m width=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_width\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m height=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_height\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m title=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_title\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[29], line 45\u001b[0m, in \u001b[0;36msave_image_to_file\u001b[0;34m(img_src)\u001b[0m\n\u001b[1;32m     42\u001b[0m filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_directory, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename_safe_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Download and save the image\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_src\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m log_message(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage saved as: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mmrag/lib/python3.11/urllib/request.py:241\u001b[0m, in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;124;03mRetrieve a URL into a temporary location on disk.\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;124;03mdata file as well as the resulting HTTPMessage object.\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    239\u001b[0m url_type, path \u001b[38;5;241m=\u001b[39m _splittype(url)\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mclosing(\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[1;32m    242\u001b[0m     headers \u001b[38;5;241m=\u001b[39m fp\u001b[38;5;241m.\u001b[39minfo()\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;66;03m# Just return the local path and the \"headers\" for file://\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;66;03m# URLs. No sense in performing a copy unless requested.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mmrag/lib/python3.11/urllib/request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mmrag/lib/python3.11/urllib/request.py:519\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    516\u001b[0m     req \u001b[38;5;241m=\u001b[39m meth(req)\n\u001b[1;32m    518\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murllib.Request\u001b[39m\u001b[38;5;124m'\u001b[39m, req\u001b[38;5;241m.\u001b[39mfull_url, req\u001b[38;5;241m.\u001b[39mdata, req\u001b[38;5;241m.\u001b[39mheaders, req\u001b[38;5;241m.\u001b[39mget_method())\n\u001b[0;32m--> 519\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[1;32m    522\u001b[0m meth_name \u001b[38;5;241m=\u001b[39m protocol\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_response\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mmrag/lib/python3.11/urllib/request.py:536\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m    535\u001b[0m protocol \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mtype\n\u001b[0;32m--> 536\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_open\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m    537\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_open\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mmrag/lib/python3.11/urllib/request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mmrag/lib/python3.11/urllib/request.py:1391\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[0;32m-> 1391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHTTPSConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mmrag/lib/python3.11/urllib/request.py:1352\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[1;32m   1351\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n\u001b[0;32m-> 1352\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   1354\u001b[0m     h\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mmrag/lib/python3.11/http/client.py:1395\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1393\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1394\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1395\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1396\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1397\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mmrag/lib/python3.11/http/client.py:325\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 325\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mmrag/lib/python3.11/http/client.py:286\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 286\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mmrag/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mmrag/lib/python3.11/ssl.py:1314\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1311\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1312\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1313\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mmrag/lib/python3.11/ssl.py:1166\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1166\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1168\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Log function to save logs to a file with date and time\n",
    "def log_message(message, filename=\"docs/mmgamerag.log\"):\n",
    "    \"\"\"\n",
    "    Saves the provided log message to a file with the current date and time.\n",
    "    \"\"\"\n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    with open(filename, \"a\", encoding=\"utf-8\") as log_file:\n",
    "        log_file.write(f\"[{current_time}] {message}\\n\")\n",
    "\n",
    "\n",
    "# Base64 encode the image\n",
    "def get_base64_encoded_image(image_url):\n",
    "    \"\"\"\n",
    "    Fetches the image from the given URL and returns its Base64 encoded string.\n",
    "    \"\"\"\n",
    "    return ''  # Temporarily returning an empty string for base64\n",
    "\n",
    "# Save content to file, for back up only, structured data is saved in JSON file.\n",
    "def save_content_to_file(content, filename):\n",
    "    \"\"\"\n",
    "    Saves the provided content to a file.\n",
    "    \"\"\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(content)\n",
    "    log_message(f\"Content successfully saved to {filename}\")\n",
    "\n",
    "def save_image_to_file(img_src):\n",
    "    \"\"\"\n",
    "    Save the image from the provided URL to a specified directory with a safe filename.\n",
    "    \n",
    "    Parameters:\n",
    "    img_src (str): The URL of the image to be saved.\n",
    "    \"\"\"\n",
    "    # Clean the URL to make it filename-safe\n",
    "    filename_safe_url = img_src.replace(\":\", \"=\").replace(\"/\", \"|\")\n",
    "    \n",
    "    # Specify the save path\n",
    "    save_directory = \"docs/rawdata/img\"\n",
    "    os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "    # Define the filename\n",
    "    filename = os.path.join(save_directory, f\"{filename_safe_url}\")\n",
    "    \n",
    "    # Download and save the image\n",
    "    urllib.request.urlretrieve(img_src, filename)\n",
    "    log_message(f\"Image saved as: {filename}\")\n",
    "\n",
    "\n",
    "# Save content to JSON file in a specified format\n",
    "def save_data_json_with_format(content, filename):\n",
    "    \"\"\"\n",
    "    Saves the provided content to a JSON file with specified indentation format.\n",
    "    Ensures the content is appended correctly to an existing JSON array.\n",
    "    \"\"\"\n",
    "    # Check if the file exists and load its content if it does\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'r', encoding=\"utf-8\") as json_file:\n",
    "            try:\n",
    "                existing_data = json.load(json_file)\n",
    "            except json.JSONDecodeError:\n",
    "                existing_data = []\n",
    "    else:\n",
    "        existing_data = []\n",
    "    \n",
    "    # Append new content to the existing data\n",
    "    existing_data.extend(content)\n",
    "    \n",
    "    # Save the updated data back to the file\n",
    "    with open(filename, 'w', encoding=\"utf-8\") as json_file:\n",
    "        json.dump(existing_data, json_file, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    log_message(f\"JSON content successfully saved to {filename}\")   \n",
    "\n",
    "# Extract image description from the image\n",
    "def get_image_description(image_src, content_before_image_str, content_after_image_str):\n",
    "    image_description=''\n",
    "    # return image_description\n",
    "    imgdesllm = ChatOpenAI(name=\"image_des_llm\", model=\"gpt-4o-mini\")\n",
    "\n",
    "    # imgdesllm = ChatNVIDIA(\n",
    "    # model=\"meta/llama-3.2-90b-vision-instruct\",\n",
    "    # api_key=\"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\", \n",
    "    # top_p=0.7,\n",
    "    # )\n",
    "\n",
    "\n",
    "    # ~~~~~~~~~~~Transfer image src to base64 and then send to llm for description handling faster.\n",
    "    # Clean the URL to make it filename-safe\n",
    "    filename_safe_url = image_src.replace(\":\", \"=\").replace(\"/\", \"|\")\n",
    "    filename_safe_url = 'docs/rawdata/img/' + filename_safe_url\n",
    "    from PIL import Image\n",
    "    image_open = Image.open(filename_safe_url)\n",
    "\n",
    "    # Convert image to base64\n",
    "    buffered = BytesIO()\n",
    "    image_open.save(buffered, format=\"JPEG\")\n",
    "    img_base64 = base64.b64encode(buffered.getvalue()).decode()\n",
    "    # ~~~~~~~~~~~\n",
    "\n",
    "    message = HumanMessage(\n",
    "    content=[\n",
    "        {\"type\": \"text\", \"text\": f\"用300字以内的中文描述这张图片（以下简称此图）的内容。并将此图的上文和下文中的内容详细总结到此图的描述中。此图的上文：\\n{content_before_image_str}\\n此图的下文：\\n{content_after_image_str}。如果在此图的上文和下文中找到了图片标签<img>，则按原来的顺序注明所有图片的src（从标签<img>中获取），不要遗漏任何的图片标签。严格遵循以下格式：\\n 图片描述。\\n上文图片的src:\\n 此图片的src:\\n{image_src}\\n 下文图片的src:\\n\"},\n",
    "        # {\"type\": \"image_url\", \"image_url\": {\"url\": image_src}},\n",
    "        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"}},\n",
    "        ],\n",
    "    )\n",
    "    img_response = imgdesllm.invoke([message])\n",
    "    print(f'\\n------------------------\\n{img_response.content}')\n",
    "    image_description=img_response.content\n",
    "\n",
    "    return image_description\n",
    "\n",
    "def get_all_image_description(file_path):\n",
    "    \"\"\"\n",
    "    Reads the JSON file, updates the image description for each item, and writes the updated data back to a temporary file\n",
    "    in batches. After processing all items, the temporary file is renamed to replace the original file.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): The path to the JSON file.\n",
    "    \"\"\"\n",
    "    # Log the start of the process\n",
    "    log_message(f\"=== Starting to get image description from file: {file_path} === \")\n",
    "\n",
    "    # Read the JSON file\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "        log_message(f\"Successfully loaded the JSON file: {file_path}\")\n",
    "    except Exception as e:\n",
    "        log_message(f\"Error loading the JSON file: {file_path}. Error: {e}\")\n",
    "        return\n",
    "    \n",
    "    temp_file_path = file_path + \".tmp\"\n",
    "    batch_size = 100  # Set batch size to 100\n",
    "\n",
    "    # Process the data in batches of 100 items\n",
    "    for index, item in enumerate(tqdm(data, desc=\"Processing images\")):\n",
    "        try:\n",
    "            # Extract relevant fields\n",
    "            image_src = item.get('src', '')\n",
    "            content_before_image_str = item.get('content_before_image', '')\n",
    "            content_after_image_str = item.get('content_after_image', '')\n",
    "            \n",
    "            # Call the image description function\n",
    "            image_description = get_image_description(image_src, content_before_image_str, content_after_image_str)\n",
    "            \n",
    "            # Update the image_description field in the item\n",
    "            item['image_description'] = image_description\n",
    "            \n",
    "            # Every 100 items, write the data to the temporary file\n",
    "            if (index + 1) % batch_size == 0:\n",
    "                with open(temp_file_path, 'w', encoding='utf-8') as temp_file:\n",
    "                    json.dump(data, temp_file, ensure_ascii=False, indent=4)\n",
    "                log_message(f\"Batch write: Processed and wrote {index+1}/{len(data)} items.\")\n",
    "        except Exception as e:\n",
    "            log_message(f\"Error processing item {index+1} and src {image_src}. Error: {e}\")\n",
    "\n",
    "    # Write remaining items if total number is not a multiple of batch_size\n",
    "    if len(data) % batch_size != 0:\n",
    "        try:\n",
    "            with open(temp_file_path, 'w', encoding='utf-8') as temp_file:\n",
    "                json.dump(data, temp_file, ensure_ascii=False, indent=4)\n",
    "            log_message(f\"Final batch write: Processed and wrote all remaining items.\")\n",
    "        except Exception as e:\n",
    "            log_message(f\"Error writing final batch to temporary file: {temp_file_path}. Error: {e}\")\n",
    "\n",
    "    # Rename the temporary file to the original file after processing all items\n",
    "    try:\n",
    "        os.replace(temp_file_path, file_path)\n",
    "        log_message(f\"Successfully replaced original file with updated data: {file_path}\")\n",
    "    except Exception as e:\n",
    "        log_message(f\"Error replacing the original file with updated data: {file_path}. Error: {e}\")\n",
    "    \n",
    "    # Log the completion of the process\n",
    "    log_message(f\"=== Finished getting image description from file: {file_path} ===\")\n",
    "    \n",
    "\n",
    "\n",
    "# Extract text and images from the part with class=\"Mid2L_con\" and save to docs first, then JSON. n means how many lines of text was stored before and after each image.\n",
    "def extract_text_and_images(currenturl, tree, n=20):\n",
    "    \"\"\"\n",
    "    Extracts text and images from the part of the webpage with class=\"Mid2L_con\", \n",
    "    and splits into two parts: one with text only, and one with text + images.\n",
    "    Saves content to files first in 'docs', then processes and saves image metadata to JSON files.\n",
    "    \"\"\"\n",
    "    \n",
    "    if tree:\n",
    "\n",
    "        # Extract the page title\n",
    "        # title = tree.xpath('//title/text()') # regular title\n",
    "        # if title:\n",
    "        #     title_text = title[0].strip()\n",
    "        # else:\n",
    "        #     title_text = 'No Title'\n",
    "        \n",
    "        title_text = ''\n",
    "        # Extract title with detailed data and author info, such as \"2023-08-21 14:06:02 来源：游民星空 作者：LIN木木 编辑：LIN木木　浏览：17536\"\n",
    "        mid2L_tit_elements = tree.xpath('//div[@class=\"Mid2L_tit\"]')\n",
    "        if mid2L_tit_elements:\n",
    "            title_text = mid2L_tit_elements[0].text_content()           \n",
    "            # Remove leading and trailing whitespace and empty lines in the middle\n",
    "            lines = [line.strip() for line in title_text.splitlines() if line.strip()] \n",
    "            title_text = \"\\n\".join(lines)\n",
    "\n",
    "        # Extract the part of the page with class=\"Mid2L_con\"\n",
    "        mid2l_con = tree.xpath('//div[@class=\"Mid2L_con\"]')\n",
    "\n",
    "        # Clean the URL to make it filename-safe\n",
    "        filename_safe_url = currenturl.replace(\":\", \"=\").replace(\"/\", \"|\")\n",
    "\n",
    "        if mid2l_con:\n",
    "            text_content_list = [f\"Title: {title_text}\"]\n",
    "            text_with_images_list = [f\"Title: {title_text}\"]\n",
    "            txt_data_list = []\n",
    "            stop_extraction = False\n",
    "\n",
    "            # First pass: gather all text and image elements\n",
    "            for element in mid2l_con[0].iter():\n",
    "                if stop_extraction:\n",
    "                    break\n",
    "\n",
    "                # If it's a text node, extract the text and tail\n",
    "                if element.text and isinstance(element.tag, str):\n",
    "                    text = element.text.strip()\n",
    "                else:\n",
    "                    text = \"\"\n",
    "\n",
    "                # Also check the 'tail' for text outside the tag\n",
    "                if element.tail:\n",
    "                    tail_text = element.tail.strip()\n",
    "                else:\n",
    "                    tail_text = \"\"\n",
    "\n",
    "                # Combine text and tail_text\n",
    "                combined_text = text + \" \" + tail_text if text or tail_text else \"\"\n",
    "\n",
    "                # Append the combined text to the list if it's not empty and does not contain certain phrases\n",
    "                if combined_text:\n",
    "                    # Check if the combined text starts with \"本文由游民星空\"\n",
    "                    if combined_text.startswith(\"本文由游民星空\"):\n",
    "                        stop_extraction = True\n",
    "                    else:\n",
    "                        # Check if combined_text contains any of the unwanted phrases\n",
    "                        unwanted_phrases = [\n",
    "                            \"更多相关内容请关注\",\n",
    "                            \"责任编辑\",\n",
    "                            \"友情提示：\",\n",
    "                            \"本文是否解决了您的问题\",\n",
    "                            \"已解决\",\n",
    "                            \"未解决\",\n",
    "                            \"黑神话：悟空专区\",\n",
    "                            \"上一页\",\n",
    "                            \"下一页\"\n",
    "                        ]\n",
    "                        if not any(phrase in combined_text for phrase in unwanted_phrases):\n",
    "                            text_content_list.append(combined_text)\n",
    "                            text_with_images_list.append(combined_text)\n",
    "\n",
    "\n",
    "                # If it's an <img> tag\n",
    "                if element.tag == 'img' and not stop_extraction:\n",
    "                    img_src = element.get('src')\n",
    "                    img_data_src = element.get('data-src', img_src)  # Use data-src if available, otherwise fallback to src\n",
    "                    img_alt = element.get('alt', '')\n",
    "                    img_title = element.get('title', '')\n",
    "                    img_width = element.get('width', '')\n",
    "                    img_height = element.get('height', '')\n",
    "\n",
    "                    # Convert relative paths to absolute URLs\n",
    "                    img_src = urljoin(currenturl, img_data_src)\n",
    "\n",
    "                    img_src=img_src.replace('_S.jpg', '.jpg')\n",
    "\n",
    "                    # Save the raw image to a file\n",
    "                    save_image_to_file(img_src)\n",
    "\n",
    "                    # Replace the placeholder with the actual image tag\n",
    "                    img_tag = f'<img src=\"{img_src}\" alt=\"{img_alt}\" width=\"{img_width}\" height=\"{img_height}\" title=\"{img_title}\">'\n",
    "                    text_with_images_list.append(img_tag)\n",
    "\n",
    "            # Convert text_content_list to a single string\n",
    "            text_content_str = '\\n'.join(text_content_list)\n",
    "            text_with_images_list_str = '\\n'.join(text_with_images_list)\n",
    "\n",
    "            # Save content to docs folder first\n",
    "            text_only_filename = os.path.join(\"docs/rawdata/\", f\"{filename_safe_url}_text_only.txt\")\n",
    "            text_with_images_filename = os.path.join(\"docs/rawdata/\", f\"{filename_safe_url}_text_with_images.html\")\n",
    "            save_content_to_file(text_content_str, text_only_filename)\n",
    "            save_content_to_file(text_with_images_list_str, text_with_images_filename)                \n",
    "\n",
    "            # Get img data\n",
    "            img_data_list = []\n",
    "            for img_index, line in enumerate(text_with_images_list):\n",
    "                if line.startswith(\"<img\"):\n",
    "                    # Extract image attributes\n",
    "                    img_src = line.split('src=\"')[1].split('\"')[0]\n",
    "                    img_alt = line.split('alt=\"')[1].split('\"')[0]\n",
    "                    img_width = line.split('width=\"')[1].split('\"')[0]\n",
    "                    img_height = line.split('height=\"')[1].split('\"')[0]\n",
    "                    img_title = line.split('title=\"')[1].split('\"')[0]\n",
    "                    \n",
    "                    # Get Base64 encoded image content (currently returning empty string)\n",
    "                    img_base64 = get_base64_encoded_image(img_src)\n",
    "                    \n",
    "                    # Get n lines before and after the image\n",
    "                    content_before_image = []\n",
    "                    content_after_image = []\n",
    "                    \n",
    "                    # Extract n lines before the image, stop if another <img> tag is encountered\n",
    "                    for i in range(img_index-1, max(0, img_index-n)-1, -1):\n",
    "                        # if '<img' in text_with_images_list[i]:\n",
    "                        #     break\n",
    "                        content_before_image.append(text_with_images_list[i])\n",
    "                    content_before_image.reverse()\n",
    "                    \n",
    "                    # Extract n lines after the image, stop if another <img> tag is encountered\n",
    "                    for i in range(img_index+1, min(len(text_with_images_list), img_index+1+n)):\n",
    "                        # if '<img' in text_with_images_list[i]:\n",
    "                        #     break\n",
    "                        content_after_image.append(text_with_images_list[i])\n",
    "                    \n",
    "                    content_before_image_str = '\\n'.join(content_before_image)\n",
    "                    content_after_image_str = '\\n'.join(content_after_image)\n",
    "                    image_descrip_str = ''\n",
    "                    \n",
    "                    # Add the image metadata to the img_data_list\n",
    "                    img_data_list.append({\n",
    "                        \"src\": img_src,\n",
    "                        \"base64\": img_base64,  # Temporarily set to an empty string\n",
    "                        \"title\": img_title,\n",
    "                        \"alt\": img_alt,\n",
    "                        \"content_before_image\": content_before_image_str,\n",
    "                        \"image_description\": image_descrip_str,\n",
    "                        \"content_after_image\": content_after_image_str,\n",
    "                        \"url\": currenturl,  # Current page url\n",
    "                        \"type\": \"img\"\n",
    "                    })\n",
    "\n",
    "\n",
    "            txt_data_list.append({\n",
    "                    \"txt\": text_content_str,\n",
    "                    \"url\": currenturl,\n",
    "                    \"type\": \"text\"\n",
    "                })\n",
    "\n",
    "            # Save the entire text_content_str directly to mmtext.json\n",
    "            save_data_json_with_format(txt_data_list, \"docs/mmtext.json\")\n",
    "\n",
    "            # Save the image metadata to JSON as a list of objects\n",
    "            save_data_json_with_format(img_data_list, \"docs/mmimg.json\")\n",
    "\n",
    "            return f\"Content saved to files in docs and JSON files processed.\"\n",
    "        else:\n",
    "            return \"No content found with class='Mid2L_con'.\"\n",
    "    else:\n",
    "        return \"Failed to fetch content.\"\n",
    "\n",
    "# Load existing links from the JSON file\n",
    "def load_existing_links(filename):\n",
    "    \"\"\"\n",
    "    Loads existing links from the specified JSON file.\n",
    "    If the file doesn't exist, it returns an empty list.\n",
    "    \"\"\"\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'r', encoding=\"utf-8\") as json_file:\n",
    "            return json.load(json_file)\n",
    "    return []\n",
    "    \n",
    "\n",
    "\n",
    "# Global variable to track new links added across function calls\n",
    "new_link_count = 0\n",
    "\n",
    "def save_link_to_json(new_link, filename=\"docs/links.json\"):\n",
    "    \"\"\"\n",
    "    Saves the provided link to the specified JSON file.\n",
    "    If the link contains .shtml?, it removes the string after .shtml.\n",
    "    If the link already exists, it returns False.\n",
    "    If the link is new and added, it returns True.\n",
    "    Logs the added link along with the updated total count of new links.\n",
    "    \"\"\"\n",
    "    global new_link_count  # Use the global counter for new links\n",
    "\n",
    "    # Check if the new_link contains '.shtml?'\n",
    "    if \".shtml?\" in new_link:\n",
    "        new_link = new_link.split(\".shtml?\")[0] + \".shtml\"\n",
    "    \n",
    "    links = load_existing_links(filename)\n",
    "    \n",
    "    if new_link not in links:\n",
    "        links.append(new_link)\n",
    "        new_link_count += 1  # Increment the global counter for a new link\n",
    "        with open(filename, 'w', encoding=\"utf-8\") as json_file:\n",
    "            json.dump(links, json_file, indent=4, ensure_ascii=False)\n",
    "        log_message(f\"New link added to links.json: {new_link}. Total links: {new_link_count}\")\n",
    "        return True\n",
    "    else:\n",
    "        log_message(f\"Link already in links.json: {new_link}\")\n",
    "        return False    \n",
    "\n",
    "\n",
    "\n",
    "# Global variable to track new URLs added across function calls\n",
    "new_url_count = 0\n",
    "\n",
    "# Save crawled URLs to the JSON file and count added URLs\n",
    "def save_crawled_url_to_json(new_url, filename=\"docs/crawled_urls.json\"):\n",
    "    \"\"\"\n",
    "    Saves the provided URL to the specified JSON file.\n",
    "    If the URL already exists, it returns False.\n",
    "    If the URL is new and added, it returns True.\n",
    "    Logs the added URL along with the updated total count of new URLs.\n",
    "    \"\"\"\n",
    "    global new_url_count  # Use the global counter for new URLs\n",
    "\n",
    "    # Check if the file exists and load existing URLs\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'r', encoding=\"utf-8\") as json_file:\n",
    "            urls = json.load(json_file)\n",
    "    else:\n",
    "        urls = [] \n",
    "    \n",
    "    log_message(f\"-----------------------------------------------------------\")\n",
    "    \n",
    "    # Check if the URL is new\n",
    "    if new_url not in urls:\n",
    "        urls.append(new_url)\n",
    "        new_url_count += 1  # Increment the global counter for a new URL\n",
    "        with open(filename, 'w', encoding=\"utf-8\") as json_file:\n",
    "            json.dump(urls, json_file, indent=4, ensure_ascii=False)\n",
    "        log_message(f\"New crawled url added to crawled_urls.json: {new_url}. Total urls: {new_url_count}\")\n",
    "        return True \n",
    "    else:\n",
    "        log_message(f\"Url already in crawled_urls.json: {new_url}\")\n",
    "        return False\n",
    "    \n",
    "\n",
    "# Check if the link exists in the JSON file\n",
    "def check_link_in_json(new_link, filename=\"docs/links.json\"):\n",
    "    \"\"\"\n",
    "    Checks if the provided link exists in the specified JSON file.\n",
    "    If the link contains .shtml?, it removes the string after .shtml.\n",
    "    Returns True if the link is found, otherwise False.\n",
    "    \"\"\"\n",
    "    # Check if the new_link contains '.shtml?'\n",
    "    if \".shtml?\" in new_link:\n",
    "        new_link = new_link.split(\".shtml?\")[0] + \".shtml\"\n",
    "    \n",
    "    # Load existing links from the JSON file\n",
    "    links = load_existing_links(filename)\n",
    "    \n",
    "    # Return True if the link is found, otherwise False\n",
    "    if new_link in links:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "# Send request with retry mechanism\n",
    "def fetch_url_with_retries(url, max_retries=2):\n",
    "    \"\"\"\n",
    "    Attempts to fetch content from the given URL, retrying up to max_retries times.\n",
    "    If the request fails, it waits for 1 second before retrying.\n",
    "    \"\"\"\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            response = requests.get(url, timeout=3)  # Set timeout to 3 seconds\n",
    "            \n",
    "            # If the status code is 200, the request was successful, return the content\n",
    "            if response.status_code == 200:\n",
    "                log_message(f\"Success on attempt {retries + 1} for {url}\")\n",
    "                return response.content\n",
    "            \n",
    "            # If the status code is not 200, log the failure reason\n",
    "            else:\n",
    "                log_message(f\"Attempt {retries + 1} failed with status code {response.status_code}\")\n",
    "        \n",
    "        except requests.RequestException as e:\n",
    "            # Capture request exceptions like timeout or connection errors\n",
    "            log_message(f\"Attempt {retries + 1} failed with error: {e}\")\n",
    "        \n",
    "        # Increment retry count\n",
    "        retries += 1\n",
    "        \n",
    "        # Wait for 1 second before retrying\n",
    "        time.sleep(1)\n",
    "\n",
    "    # If max retries are exceeded, return None or handle the error accordingly\n",
    "    log_message(f\"Failed to fetch the URL after {max_retries} attempts.\")\n",
    "    return None\n",
    "\n",
    "# New function to crawl the webpage and its linked pages up to a given depth\n",
    "def crawl_and_extract(url, keyword=\"黑神话\", linkdepth=2):\n",
    "    \"\"\"\n",
    "    Crawls the webpage starting from the given URL, and checks for links within the page.\n",
    "    If a page contains the term specified in 'keyword' in either \"Mid2L_con\" class or in the title,\n",
    "    or in the whole HTML content, it saves the link in 'links.json'.\n",
    "    Crawls up to the given linkdepth (including the original URL).\n",
    "    \"\"\"\n",
    "    def crawl(url, current_depth, max_depth, pbar):\n",
    "\n",
    "        # Check if the link has been crawled\n",
    "        if save_crawled_url_to_json(url) == False:\n",
    "            return\n",
    "\n",
    "\n",
    "        # Check if the link exists in the JSON file and it is in the max depth, if yes, just return.\n",
    "        if check_link_in_json(url) == True and current_depth == max_depth:\n",
    "            log_message(f\"Link found in links.json: {url} (Depth: {current_depth})\")\n",
    "            return\n",
    "        \n",
    "        # Fetch the page content\n",
    "        html_content = fetch_url_with_retries(url)\n",
    "        if not html_content:\n",
    "            log_message(f\"Failed to fetch content for {url} (Depth: {current_depth})\")\n",
    "            return\n",
    "        \n",
    "        # Parse the HTML using lxml\n",
    "        tree = html.fromstring(html_content)\n",
    "\n",
    "        # Check if class=\"Mid2L_con\" or title contains the keyword\n",
    "        mid2l_con_elements = tree.xpath('//div[@class=\"Mid2L_con\"]')\n",
    "        title_elements = tree.xpath('//title/text()')\n",
    "        \n",
    "        # Check if keyword exists in Mid2L_con or Title\n",
    "        mid2l_con_text = mid2l_con_elements[0].text_content() if mid2l_con_elements else \"\"\n",
    "        title_text = title_elements[0] if title_elements else \"\"\n",
    "        \n",
    "        links = []\n",
    "\n",
    "        if mid2l_con_text:\n",
    "            if keyword in mid2l_con_text or keyword in title_text:\n",
    "                if current_depth == 0: current_depth = 1   # 0 is for url without mid2l_con, so we set it to 1\n",
    "                log_message(f\"Found '{keyword}' in Mid2L_con or Title at {url} (Depth: {current_depth})\")\n",
    "                linkexist = save_link_to_json(url)  # Save the link to JSON\n",
    "                if linkexist == True:\n",
    "                    extract_text_and_images(url, tree)\n",
    "                if current_depth < max_depth:\n",
    "                    # Get all links on the page\n",
    "                    alinks = tree.xpath('//div[@class=\"Mid2L_con\"]//a[@href]/@href')\n",
    "                    links = [urljoin(url, link) for link in alinks if link.startswith(('http', '/'))]\n",
    "                    links = list(set(links))\n",
    "                    # Remove unwanted link, links starting with 'javascript:', and those ending with '.jpg' or '.png'\n",
    "                    unwanted_link = \"\" # \"https://www.gamersky.com/z/bmwukong/\"\n",
    "                    filtered_links = [link for link in links if link != unwanted_link and not link.startswith('javascript:') and not link.endswith(('.jpg', '.png'))]\n",
    "                    links = filtered_links\n",
    "                    log_message(f\"Found {len(links)} links on {url} (Depth: {current_depth}). Crawling deeper...\")\n",
    "\n",
    "        else:\n",
    "            # If not found in Mid2L_con, check the full HTML content\n",
    "            if keyword in title_text: \n",
    "                current_depth = 0\n",
    "                log_message(f\"Found '{keyword}' in full HTML at {url} (Depth: {current_depth})\")\n",
    "            # if keyword in html_content.decode('utf-8', errors='ignore'):                \n",
    "                linkexist = save_link_to_json(url)  # Save the link to JSON\n",
    "                if linkexist == True:\n",
    "                    pass\n",
    "                    # extract_text_and_images(url, tree)   # Don't extract if it is just an overview\n",
    "\n",
    "                if current_depth < max_depth:\n",
    "                    # Get all links on the page\n",
    "                    alinks = tree.xpath('//a[@href]/@href')\n",
    "                    links = [urljoin(url, link) for link in alinks if link.startswith(('http', '/'))]\n",
    "                    links = list(set(links))\n",
    "                    # Remove unwanted link, links starting with 'javascript:', and those ending with '.jpg' or '.png'\n",
    "                    unwanted_link = \"\" # \"https://www.gamersky.com/z/bmwukong/\"\n",
    "                    filtered_links = [link for link in links if link != unwanted_link and not link.startswith('javascript:') and not link.endswith(('.jpg', '.png'))]\n",
    "                    links = filtered_links\n",
    "                    log_message(f\"Found {len(links)} links on {url} (Depth: {current_depth}). Crawling deeper...\")\n",
    "            else:\n",
    "                log_message(f\"No '{keyword}' found at {url} (Depth: {current_depth})\")\n",
    "\n",
    "        \n",
    "        if current_depth < max_depth and links:\n",
    "            current_depth = current_depth + 1\n",
    "            # Recursively crawl the found links, with increased depth\n",
    "            for link in tqdm(links, desc=f\"Crawling depth {current_depth}/{max_depth}\", leave=False, position=1, dynamic_ncols=True):\n",
    "                crawl(link, current_depth, max_depth, pbar)\n",
    "                pbar.update(1)\n",
    "\n",
    "\n",
    "\n",
    "    # Write the start message\n",
    "    log_message(\"=== Crawl Start ===\")\n",
    "\n",
    "    with tqdm(total=100, desc=\"Crawling\", position=0, dynamic_ncols=True) as pbar:\n",
    "        crawl(url, current_depth=0, max_depth=linkdepth, pbar=pbar)\n",
    "\n",
    "    # Write the end message with two empty lines\n",
    "    log_message(\"=== Crawl End ===\\n\\n\")\n",
    "\n",
    "# Test URL and Keyword\n",
    "url = \"https://www.gamersky.com/z/bmwukong/\"\n",
    "keyword = \"黑神话\"\n",
    "\n",
    "crawl_and_extract(url, keyword=keyword, linkdepth=2) # Crawl the URL and its linked pages up to a depth\n",
    "# get_all_image_description('docs/mmimg.json') # Get image description for all images in the JSON file\n",
    "\n",
    "# image_src='http://img1.gamersky.com/image2024/08/20240819_qy_372_15/image077.jpg'\n",
    "# content_before_image_str =''\n",
    "# content_after_image_str=''\n",
    "# content_before_image_str='Title: 《黑神话悟空》珍玩图鉴 珍玩获取方法及效果一览\\n2024-08-20 10:19:28 来源：游民星空[原创] 作者：瑞破受气包  我要投稿\\n第13页：特品-金棕衣 \\n展开 \\n特品-金棕衣 \\n获取方法：【 \\n可能是焦面鬼王概率掉落 】。从第三回【极乐谷-长生大道】土地庙出发，进入土地庙前方木门之后往左前方走，击败前方雪地上的焦面鬼王（超大巨人）即可获得。具体路线请参考下文。\\n<img src=\\\"http://img1.gamersky.com/image2024/08/20240819_qy_372_15/image073.jpg\\\" alt=\\\"游民星空\\\" width=\\\"\\\" height=\\\"\\\" title=\\\"\\\">\\n<img src=\\\"http://img1.gamersky.com/image2024/08/20240819_qy_372_15/image075.jpg\\\" alt=\\\"游民星空\\\" width=\\\"\\\" height=\\\"\\\" title=\\\"\\\">\\n从第三回【极乐谷-长生大道】土地庙出发，进入土地庙前方木门之后往左前方走。 '\n",
    "# content_after_image_str='继续沿路前进，在拐弯处往右走，可以看到一大片雪地，还有一个超大巨人，巨人就是焦面鬼王，击杀即可获得。 \\n<img src=\\\"http://img1.gamersky.com/image2024/08/20240819_qy_372_15/image079.jpg\\\" alt=\\\"游民星空\\\" width=\\\"\\\" height=\\\"\\\" title=\\\"\\\">\\n11 \\n12 \\n13 \\n14 \\n15 \\n16 \\n17 \\n18 \\n19 \\n20 \\n21 \\n0 \\n0 \\n文章内容导航 \\n第1页：上品-猫睛宝串 \\n第2页：上品-玛瑙罐 \\n第3页：上品-不求人 \\n第4页：上品-砗磲佩 '\n",
    "\n",
    "# get_image_description(image_src, content_before_image_str, content_after_image_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized an empty vectorstore in vectorstore/chromadb-mmgamerag\n",
      "Quantity of existing_urls: 0\n",
      "Added 38 new text documents.\n",
      "Quantity of existing_srcs: 0\n",
      "Added 139 new img documents.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv,find_dotenv\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.schema.document import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "load_dotenv(find_dotenv()) \n",
    "\n",
    "# Preparation of documents for RAG-------------------------\n",
    "# Vectorstore, for retrieval\n",
    "embedding_model=OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "vectorstore_path = \"vectorstore/chromadb-mmgamerag\"\n",
    "if os.path.exists(vectorstore_path):\n",
    "    print(f\"Loaded vectorstore from disk: {vectorstore_path}\")\n",
    "else:\n",
    "    # Initialize an empty vectorstore and persist to disk\n",
    "    print(f\"Initialized an empty vectorstore in {vectorstore_path}\")\n",
    "\n",
    "vectorstore = Chroma(\n",
    "                embedding_function=embedding_model,\n",
    "                persist_directory=vectorstore_path,\n",
    "                ) \n",
    "\n",
    "def add_text_documents_to_vectorstore(vectorstore, documents):\n",
    "    # Retrieve existing documents from the vectorstore\n",
    "    existing_docs = vectorstore.get()\n",
    "    \n",
    "    existing_urls = [metadata['url'] for metadata in existing_docs['metadatas']] #???metadata\n",
    "    print(f\"Quantity of existing_urls: {len(existing_urls)}\")\n",
    "    # print(existing_urls)\n",
    "    # Filter out documents that already exist based on URL\n",
    "    new_documents = [doc for doc in documents if doc.metadata[\"url\"] not in existing_urls]\n",
    "\n",
    "    if new_documents:\n",
    "        vectorstore.add_documents(new_documents)\n",
    "        # vectorstore.persist()  # Persist the vectorstore after adding documents\n",
    "        print(f\"Added {len(new_documents)} new text documents.\")\n",
    "    else:\n",
    "        print(\"No new text documents to add.\")\n",
    "\n",
    "\n",
    "def add_img_documents_to_vectorstore(vectorstore, documents):\n",
    "    # Retrieve existing documents from the vectorstore\n",
    "    existing_docs = vectorstore.get()\n",
    "    \n",
    "    # Use `get` to avoid KeyError if some metadata does not have 'src'\n",
    "    existing_srcs = [metadata.get('src') for metadata in existing_docs['metadatas'] if 'src' in metadata]\n",
    "    print(f\"Quantity of existing_srcs: {len(existing_srcs)}\")\n",
    "    # print(existing_srcs)\n",
    "    \n",
    "    # Filter out documents that already exist based on src\n",
    "    new_documents = [doc for doc in documents if doc.metadata.get(\"src\") not in existing_srcs]\n",
    "\n",
    "    if new_documents:\n",
    "        vectorstore.add_documents(new_documents)\n",
    "        # vectorstore.persist()  # Persist the vectorstore after adding documents\n",
    "        print(f\"Added {len(new_documents)} new img documents.\")\n",
    "    else:\n",
    "        print(\"No new img documents to add.\")\n",
    "\n",
    "\n",
    "def add_txt_img():\n",
    "    txt_data_list = []\n",
    "    img_data_list = []\n",
    "\n",
    "    # Directly load and assign to txt_data_list from mmtext.json\n",
    "    with open('docs/mmtext.json', 'r', encoding='utf-8') as text_file:\n",
    "        txt_data_list = json.load(text_file)  # Assuming the JSON structure matches the required format\n",
    "\n",
    "    # Directly load and assign to img_data_list from mmimg.json\n",
    "    with open('docs/mmimg.json', 'r', encoding='utf-8') as img_file:\n",
    "        img_data_list = json.load(img_file)  # Assuming the JSON structure matches the required format\n",
    "\n",
    "    # Add texts\n",
    "    mmtexts = [\n",
    "        Document(page_content=item['txt'], metadata={\"url\": item['url'], \"type\": item['type']})\n",
    "        for item in txt_data_list\n",
    "    ]\n",
    "\n",
    "    # Add documents and save to vectorstore\n",
    "    add_text_documents_to_vectorstore(vectorstore, mmtexts)\n",
    "\n",
    "\n",
    "    # Add imgs\n",
    "    mmimgs = [\n",
    "        Document(\n",
    "            page_content=\"\\ncontent_before_image:\\n\" + item['content_before_image'] + \"\\n\\nimage_description:\\n\" + item['image_description'] + \"\\n\\ncontent_after_image:\\n\" + item['content_after_image'] + '\\n',  \n",
    "            metadata={\"url\": item['url'], \"type\": item['type'], \"src\": item['src']}\n",
    "        )\n",
    "        for item in img_data_list  # Iterate over each item in img_data_list\n",
    "    ]\n",
    "\n",
    "    # Add documents and save to vectorstore\n",
    "    add_img_documents_to_vectorstore(vectorstore, mmimgs)\n",
    "\n",
    "add_txt_img() # Add texts and images to vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "\n",
    "# retrieved_docs = retriever.invoke(\"猫睛宝串\")\n",
    "# retrieved_docs\n",
    "\n",
    "retrieved_docs = vectorstore.similarity_search_with_relevance_scores(query=\"君子牌\", k=5, filter={\"type\": \"text\"})\n",
    "\n",
    "# Iterate over retrieved_docs and extract the url, page_content, and score\n",
    "for doc, score in retrieved_docs:\n",
    "    url = doc.metadata.get('url', 'No URL found')  # Extract the URL from the metadata\n",
    "    type = doc.metadata.get('type', '') \n",
    "    page_content = doc.page_content  # Get the page content\n",
    "    # print(f\"URL: {url}\\nContent: {page_content}\\nScore: {score}\\nType: {type}\\n\")\n",
    "\n",
    "retrieved_docs = vectorstore.similarity_search_with_relevance_scores(query=\"君子牌\", k=5, filter={\"type\": \"img\"})\n",
    "\n",
    "# Iterate over retrieved_docs and extract the url, page_content, and score\n",
    "for doc, score in retrieved_docs:\n",
    "    url = doc.metadata.get('url', 'No URL found')  # Extract the URL from the metadata\n",
    "    type = doc.metadata.get('type', '') \n",
    "    src = doc.metadata.get('src', '') \n",
    "    page_content = doc.page_content  # Get the page content\n",
    "    print(f\"URL: {url}\\nSRC: {src}\\nContent: {page_content}\\nScore: {score}\\nType: {type}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Q&A with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display, Image\n",
    "\n",
    "mmgamellm = ChatOpenAI(name=\"MMGameRag\", model_name=\"gpt-4o-mini\", temperature=0.6, streaming=True)\n",
    "\n",
    "def format_docs(docs_with_scores):\n",
    "    \"\"\"\n",
    "    Formats the retrieved documents into a string with their content, URL, and score,\n",
    "    and lists them in order with numbering.\n",
    "    \"\"\"\n",
    "    formatted_docs = []\n",
    "    \n",
    "    # Iterate over the documents and their associated scores\n",
    "    for i, (doc, score) in enumerate(docs_with_scores, 1):  # Enumerate to add numbering starting from 1\n",
    "        imgsrc = doc.metadata.get('src', '')\n",
    "        if imgsrc: # Image\n",
    "            formatted_doc = (\n",
    "                f\"{i}.\\n\"\n",
    "                f\"Image Content:\\n{doc.page_content}\\n\"  # Content of the document\n",
    "                f\"Page Url: {doc.metadata.get('url', '')}\\n\"  # Assuming URL is stored in metadata\n",
    "                f\"Image Src: {doc.metadata.get('src', '')}\\n\"  # Assuming URL is stored in metadata\n",
    "                f\"Score: {score}\\n\"  # Similarity score for the document\n",
    "            )\n",
    "        else:  # Text\n",
    "            formatted_doc = (\n",
    "                f\"{i}.\\n\"\n",
    "                f\"Text Content:\\n{doc.page_content}\\n\"  # Content of the document\n",
    "                f\"Page Url: {doc.metadata.get('url', '')}\\n\"  # Assuming URL is stored in metadata\n",
    "                f\"Score: {score}\\n\"  # Similarity score for the document\n",
    "            )\n",
    "        formatted_docs.append(formatted_doc)  # Add formatted document to the list\n",
    "    \n",
    "    return \"\\n\".join(formatted_docs)  # Join all formatted documents into a single string\n",
    "\n",
    "# Prompt for code generation\n",
    "prompt_template = \"\"\"你是《黑神话：悟空》这款游戏的AI助手，根据Question和Context专门为玩家提供详尽的游戏攻略并以Markdown的格式输出.请注意：\n",
    "1. 在Image中找到与Question和Answer最相关的图像。每个Image都有Text before image，Image descriptioin和Text after image，可以用来判断这个Image应该被插入到与文本答案最匹配的上下文的哪个段落当中。格式如下：\n",
    "    \n",
    "    文本答案段落\n",
    "    [![](图像1的Src)](图像1的Url)\n",
    "    文本答案段落\n",
    "    [![](图像2的Src)](图像2的Url)\n",
    "    文本答案段落\n",
    "    ...\n",
    "\n",
    "2. 在输出答案的最后，根据问题找到context中的最相关的几个参考文档，并列出Url链接，以供用户参考原始文档。\n",
    "\n",
    "Question: \n",
    "{question}\n",
    "\n",
    "Context: \n",
    "{context}\n",
    "\n",
    "Image:\n",
    "{image}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt_code = ChatPromptTemplate.from_template(prompt_template)\n",
    "\n",
    "chain = (\n",
    "    prompt_code\n",
    "    | mmgamellm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "gamer_question = \"黑神话一共有多少上品珍宝？举几个例子\"\n",
    "context_retrieval = format_docs(vectorstore.similarity_search_with_score(query=gamer_question, k=5, filter={\"type\": \"text\"}))\n",
    "# print(context_retrieval + \"\\n------------------------\\n\")\n",
    "img_retrieval = format_docs(vectorstore.similarity_search_with_score(query=gamer_question, k=5, filter={\"type\": \"img\"}))\n",
    "# print(img_retrieval + \"\\n------------------------\\n\")\n",
    "result = chain.invoke({\n",
    "    \"question\": gamer_question, \n",
    "    \"context\": context_retrieval,\n",
    "    \"image\": img_retrieval\n",
    "})\n",
    "\n",
    "\n",
    "display(Markdown(result))\n",
    "# display(Image(url=\"http://img1.gamersky.com/image2024/08/20240819_qy_372_15/image001_S.jpg\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
