{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with RecursiveUrlLoader (discarded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_community.document_loaders import RecursiveUrlLoader\n",
    "\n",
    "# Extracts text content from HTML, removing extra newlines and formatting it for readability.\n",
    "def bs4_extractor(html: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts clean text from the given HTML content, removing extra newlines for better readability.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n",
    "\n",
    "# Extracts the HTML content, preserving text and <img> tags, and placing the image in the text where found.\n",
    "def extract_text_with_images(html: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts both text and <img> tags from the given HTML content, placing images within the text where found.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    \n",
    "    output = \"\"\n",
    "    for element in soup.descendants:\n",
    "        if element.name not in ['script', 'style'] and isinstance(element, str):\n",
    "            output += element.strip() + \"\\n\\n\"\n",
    "        elif element.name == \"img\":\n",
    "            img_tag = f'<img src=\"{element.get(\"src\")}\", data-src=\"{element.get(\"data-src\")}\", alt=\"{element.get(\"alt\")}\", width=\"{element.get(\"width\")}\", height=\"{element.get(\"height\")}\">\\n\\n'\n",
    "            output += img_tag\n",
    "    \n",
    "    return re.sub(r\"\\n\\n+\", \"\\n\", output).strip()\n",
    "\n",
    "# Saves the given content to a file with the specified filename.\n",
    "def save_to_file(content: str, filename: str) -> None:\n",
    "    \"\"\"\n",
    "    Saves the content to a file with the given filename.\n",
    "    \"\"\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(content)\n",
    "\n",
    "# Process the loaded content and save the results\n",
    "def process_html_content(html_content: str, doc_i) -> None:\n",
    "    \"\"\"\n",
    "    Processes the HTML content by extracting text and images, and saves the results into files.\n",
    "    It also extracts the title and uses it to name the saved files.\n",
    "    \"\"\"\n",
    "    # Use BeautifulSoup to extract the title\n",
    "    soup = BeautifulSoup(html_content, \"lxml\")\n",
    "    title = soup.title.string if soup.title else \"untitled\"\n",
    "    \n",
    "    # Clean title for valid filename usage\n",
    "    safe_title = re.sub(r'[\\/:*?\"<>|]', \"_\", title)\n",
    "    \n",
    "    # Extract text content\n",
    "    extracted_text = bs4_extractor(html_content)\n",
    "    \n",
    "    # Extract text with images\n",
    "    text_with_images = extract_text_with_images(html_content)\n",
    "    \n",
    "    # Save files using the title as part of the filename\n",
    "    save_to_file(html_content, f\"{safe_title}[{doc_i}]_original_html.txt\")\n",
    "    save_to_file(extracted_text, f\"{safe_title}[{doc_i}]_text.txt\")\n",
    "    save_to_file(text_with_images, f\"{safe_title}[{doc_i}]_text_with_images.txt\")\n",
    "    \n",
    "    print(f\"Files saved with the title '{safe_title}'\")\n",
    "\n",
    "# Define the URL to be processed\n",
    "currenturl = \"https://www.gamersky.com/z/bmwukong/1314156_195585/\"\n",
    "\n",
    "# Use RecursiveUrlLoader without an extractor to get the original HTML content\n",
    "loader_html = RecursiveUrlLoader(\n",
    "    currenturl,\n",
    "    max_depth=10,\n",
    "    use_async=False,\n",
    "    extractor=None,  # No extractor here to get the original HTML\n",
    "    metadata_extractor=None,\n",
    "    exclude_dirs=(),\n",
    "    timeout=10,\n",
    "    check_response_status=True,\n",
    "    continue_on_failure=True,\n",
    "    prevent_outside=True,\n",
    "    base_url=None,\n",
    ")\n",
    "\n",
    "# Load the original HTML content\n",
    "docs_html = loader_html.load()\n",
    "\n",
    "# Process all documents loaded from RecursiveUrlLoader\n",
    "if docs_html and len(docs_html) > 0:\n",
    "    for i, doc in enumerate(docs_html):\n",
    "        html_content = doc.page_content\n",
    "        \n",
    "        # Optional: Include document index as part of the filename to differentiate files\n",
    "        print(f\"Processing document {i + 1}/{len(docs_html)}\")\n",
    "\n",
    "        # Process the HTML content by extracting text and images for each document\n",
    "        process_html_content(html_content, i)\n",
    "else:\n",
    "    print(\"Failed to load any content from the URL.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "\n",
    "display(Markdown(html_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import RecursiveUrlLoader\n",
    "\n",
    "# 定义要抓取的初始 URL\n",
    "currenturl = \"https://www.gamersky.com/z/bmwukong/\"\n",
    "\n",
    "# 配置 RecursiveUrlLoader\n",
    "loader = RecursiveUrlLoader(\n",
    "    currenturl,\n",
    "    max_depth=2,  # 设置递归抓取深度，例如 3 表示抓取当前页面及其链接的两级页面\n",
    "    use_async=False,  # 是否异步抓取\n",
    "    extractor=None,  # 提取器设为 None 以获取原始 HTML\n",
    "    metadata_extractor=None,  # 不使用元数据提取器\n",
    "    exclude_dirs=(),  # 可选，排除不需要抓取的目录\n",
    "    timeout=5,  # 每个页面的抓取超时时间\n",
    "    check_response_status=True,  # 是否检查 HTTP 响应状态码\n",
    "    continue_on_failure=True,  # 是否在遇到错误时继续抓取\n",
    "    prevent_outside=False,  # 防止抓取超出指定 URL 域名或目录的链接\n",
    "    base_url=currenturl,  # 确保只抓取从这个 URL 开始的页面\n",
    ")\n",
    "\n",
    "# 加载文档，返回一个包含所有递归抓取到页面的列表\n",
    "docs = loader.load()\n",
    "\n",
    "# 处理抓取到的文档\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"\\n-----------------\")\n",
    "    print(f\"Document {i+1}:\")  \n",
    "    print(doc.page_content.__len__())  # 输出每个文档的内容\n",
    "    print(doc.metadata)  # 输出每个文档的内容\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Complete page scrwling and data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from lxml import html, etree\n",
    "from urllib.parse import urljoin\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm  # Progress bar\n",
    "import json\n",
    "import urllib.request\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI \n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from io import BytesIO\n",
    "import base64\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv()) \n",
    "from langchain_together import ChatTogether\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling:   0%|          | 0/100 [00:00<?, ?it/s]/var/folders/03/4yvvy5rx6c97ntf957y4f7fw0000gn/T/ipykernel_73526/3852504055.py:226: FutureWarning: The behavior of this method will change in future versions. Use specific 'len(elem)' or 'elem is not None' test instead.\n",
      "  if tree:\n",
      "Crawling: 125653it [2:07:32, 16.42it/s]                   \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Log function to save logs to a file with date and time\n",
    "def log_message(message, filename=\"docs/mmgamerag.log\"):\n",
    "    \"\"\"\n",
    "    Saves the provided log message to a file with the current date and time.\n",
    "    \"\"\"\n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    from datetime import date\n",
    "    current_date = date.today()\n",
    "    filename=f\"docs/mmgamerag_{current_date}.log\"\n",
    "\n",
    "    with open(filename, \"a\", encoding=\"utf-8\") as log_file:\n",
    "        log_file.write(f\"[{current_time}] {message}\\n\")\n",
    "\n",
    "\n",
    "# Base64 encode the image\n",
    "def get_base64_encoded_image(image_url):\n",
    "    \"\"\"\n",
    "    Fetches the image from the given URL and returns its Base64 encoded string.\n",
    "    \"\"\"\n",
    "    return ''  # Temporarily returning an empty string for base64\n",
    "\n",
    "# Save content to file, for back up only, structured data is saved in JSON file.\n",
    "def save_content_to_file(content, filename):\n",
    "    \"\"\"\n",
    "    Saves the provided content to a file.\n",
    "    \"\"\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(content)\n",
    "    log_message(f\"Content successfully saved to {filename}\")\n",
    "\n",
    "def save_image_to_file(img_src):\n",
    "    \"\"\"\n",
    "    Save the image from the provided URL to a specified directory with a safe filename.\n",
    "    \n",
    "    Parameters:\n",
    "    img_src (str): The URL of the image to be saved.\n",
    "    \"\"\"\n",
    "    # Clean the URL to make it filename-safe\n",
    "    filename_safe_url = img_src.replace(\":\", \"=\").replace(\"/\", \"|\")\n",
    "    \n",
    "    # Specify the save path\n",
    "    save_directory = \"docs/rawdata/img\"\n",
    "    os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "    # Define the filename\n",
    "    filename = os.path.join(save_directory, f\"{filename_safe_url}\")\n",
    "    \n",
    "    # Download and save the image\n",
    "    urllib.request.urlretrieve(img_src, filename)\n",
    "    log_message(f\"Image saved as: {filename}\")\n",
    "\n",
    "\n",
    "# Save content to JSON file in a specified format\n",
    "def save_data_json_with_format(content, filename):\n",
    "    \"\"\"\n",
    "    Saves the provided content to a JSON file with specified indentation format.\n",
    "    Ensures the content is appended correctly to an existing JSON array.\n",
    "    \"\"\"\n",
    "    # Check if the file exists and load its content if it does\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'r', encoding=\"utf-8\") as json_file:\n",
    "            try:\n",
    "                existing_data = json.load(json_file)\n",
    "            except json.JSONDecodeError:\n",
    "                existing_data = []\n",
    "    else:\n",
    "        existing_data = []\n",
    "    \n",
    "    # Append new content to the existing data\n",
    "    existing_data.extend(content)\n",
    "    \n",
    "    # Save the updated data back to the file\n",
    "    with open(filename, 'w', encoding=\"utf-8\") as json_file:\n",
    "        json.dump(existing_data, json_file, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    log_message(f\"JSON content successfully saved to {filename}\")   \n",
    "\n",
    "# Extract image description from the image\n",
    "def get_image_description(image_src, content_before_image_str, content_after_image_str):\n",
    "    image_description=''\n",
    "    # return image_description\n",
    "\n",
    "    # imgdesllm = ChatOpenAI(name=\"image_des_llm\", model=\"gpt-4o-mini\")\n",
    "\n",
    "    imgdesllm = ChatNVIDIA(\n",
    "    model=\"meta/llama-3.2-90b-vision-instruct\",\n",
    "    api_key=\"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\"\n",
    "    )\n",
    "\n",
    "    # imgdesllm = ChatTogether(\n",
    "    # model=\"meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo\"\n",
    "    # # other params...\n",
    "    # )                 # 11b is not good enough\n",
    "\n",
    "    # ~~~~~~~~~~~Transfer image src to base64 and then send to llm for description handling faster.\n",
    "    # Clean the URL to make it filename-safe\n",
    "    filename_safe_url = image_src.replace(\":\", \"=\").replace(\"/\", \"|\")\n",
    "    filename_safe_url = 'docs/rawdata/img/' + filename_safe_url\n",
    "\n",
    "    if not os.path.exists(filename_safe_url):\n",
    "        log_message(f\"File {filename_safe_url} does not exist.\")\n",
    "        return\n",
    "\n",
    "    from PIL import Image\n",
    "    image_open = Image.open(filename_safe_url)\n",
    "\n",
    "    # Convert image to base64\n",
    "    buffered = BytesIO()\n",
    "\n",
    "    # Check the image format and save accordingly\n",
    "    if image_open.format == \"GIF\":\n",
    "        image_open.save(buffered, format=\"GIF\")\n",
    "    elif image_open.format == \"PNG\":\n",
    "        image_open.save(buffered, format=\"PNG\")\n",
    "    elif image_open.format == \"BMP\":\n",
    "        image_open.save(buffered, format=\"BMP\")\n",
    "    elif image_open.format == \"TIFF\":\n",
    "        image_open.save(buffered, format=\"TIFF\")\n",
    "    elif image_open.format == \"WEBP\":\n",
    "        image_open.save(buffered, format=\"WEBP\")\n",
    "    elif image_open.format == \"ICO\":\n",
    "        image_open.save(buffered, format=\"ICO\")\n",
    "    else:\n",
    "        # Default to JPEG if the format is not recognized\n",
    "        image_open.save(buffered, format=\"JPEG\")\n",
    "\n",
    "    img_base64 = base64.b64encode(buffered.getvalue()).decode()\n",
    "    # ~~~~~~~~~~~\n",
    "\n",
    "    message = HumanMessage(\n",
    "    content=[\n",
    "        {\"type\": \"text\", \"text\": f\"用300字以内的中文描述这张图片（以下简称此图）的内容。并将此图的上文和下文中的内容总结到此图的描述中，越靠近此图的上文和下文内容越重要越需要重点总结。此图的上文：\\n{content_before_image_str}\\n此图的下文：\\n{content_after_image_str}。\\n\\n严格遵循以下格式： 此图的上文提到..., 此图的描述为..., 此图的下文提到...。  \\n\\n\"},\n",
    "        # {\"type\": \"image_url\", \"image_url\": {\"url\": image_src}},\n",
    "        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"}},\n",
    "        ],\n",
    "    )\n",
    "    img_response = imgdesllm.invoke([message])\n",
    "    # print(f'\\n------------------------\\n{img_response.content}')\n",
    "    image_description=img_response.content\n",
    "\n",
    "    return image_description\n",
    "\n",
    "def get_all_image_description(file_path):\n",
    "    \"\"\"\n",
    "    Reads the JSON file, updates the image description for each item, and writes the updated data back to a temporary file\n",
    "    in batches. After processing all items, the temporary file is renamed to replace the original file.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): The path to the JSON file.\n",
    "    \"\"\"\n",
    "    # Log the start of the process\n",
    "    log_message(f\"=== Starting to get image description from file: {file_path} === \")\n",
    "\n",
    "    # Read the JSON file\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "        log_message(f\"Successfully loaded the JSON file: {file_path}\")\n",
    "    except Exception as e:\n",
    "        log_message(f\"Error loading the JSON file: {file_path}. Error: {e}\")\n",
    "        return\n",
    "    \n",
    "    temp_file_path = file_path + \".tmp\"\n",
    "    batch_size = 20  # Set batch size to 100\n",
    "\n",
    "    # Process the data in batches of 100 items\n",
    "    # for index, item in enumerate(tqdm(data, desc=\"Processing images\")):\n",
    "    for index, item in enumerate(tqdm(data[0:19754], desc=\"Processing images\")):\n",
    "        try:\n",
    "            time.sleep(0.05)\n",
    "            # Extract relevant fields\n",
    "            image_src = item.get('src', '')\n",
    "            content_before_image_str = item.get('content_before_image', '')\n",
    "            content_after_image_str = item.get('content_after_image', '')\n",
    "            image_description_in_file = item.get('image_description', '')\n",
    "            if image_description_in_file != \"\":\n",
    "                continue\n",
    "\n",
    "            \n",
    "            # Call the image description function\n",
    "            image_description = get_image_description(image_src, content_before_image_str, content_after_image_str)\n",
    "            \n",
    "            # Update the image_description field in the item\n",
    "            item['image_description'] = image_description\n",
    "\n",
    "            if image_description:\n",
    "                log_message(f\"Write description for index {index+0} .\")\n",
    "            \n",
    "            # Every 100 items, write the data to the temporary file\n",
    "            if (index + 1) % batch_size == 0:\n",
    "                with open(temp_file_path, 'w', encoding='utf-8') as temp_file:\n",
    "                    json.dump(data, temp_file, ensure_ascii=False, indent=4)\n",
    "                log_message(f\"Batch write: Processed and wrote {index+1}/{len(data)} items.\")\n",
    "        except Exception as e:\n",
    "            log_message(f\"Error processing item {index+1} and src {image_src}. Error: {e}\")\n",
    "\n",
    "    # Write remaining items if total number is not a multiple of batch_size\n",
    "    if len(data) % batch_size != 0:\n",
    "        try:\n",
    "            with open(temp_file_path, 'w', encoding='utf-8') as temp_file:\n",
    "                json.dump(data, temp_file, ensure_ascii=False, indent=4)\n",
    "            log_message(f\"Final batch write: Processed and wrote all remaining items.\")\n",
    "        except Exception as e:\n",
    "            log_message(f\"Error writing final batch to temporary file: {temp_file_path}. Error: {e}\")\n",
    "\n",
    "    # Rename the temporary file to the original file after processing all items\n",
    "    try:\n",
    "        os.replace(temp_file_path, file_path)\n",
    "        log_message(f\"Successfully replaced original file with updated data: {file_path}\")\n",
    "    except Exception as e:\n",
    "        log_message(f\"Error replacing the original file with updated data: {file_path}. Error: {e}\")\n",
    "    \n",
    "    # Log the completion of the process\n",
    "    log_message(f\"=== Finished getting image description from file: {file_path} ===\")\n",
    "    \n",
    "\n",
    "\n",
    "# Extract text and images from the part with class=\"Mid2L_con\" and save to docs first, then JSON. n means how many lines of text was stored before and after each image.\n",
    "def extract_text_and_images(currenturl, tree, n=20):\n",
    "    \"\"\"\n",
    "    Extracts text and images from the part of the webpage with class=\"Mid2L_con\", \n",
    "    and splits into two parts: one with text only, and one with text + images.\n",
    "    Saves content to files first in 'docs', then processes and saves image metadata to JSON files.\n",
    "    \"\"\"\n",
    "    \n",
    "    if tree:\n",
    "\n",
    "        # Extract the page title\n",
    "        title = tree.xpath('//title/text()') # regular title\n",
    "        if title:\n",
    "            title_text = title[0].strip()\n",
    "        else:\n",
    "            title_text = 'No Title'\n",
    "    \n",
    "\n",
    "        # Extract the part of the page with class=\"Mid2L_con\"\n",
    "        mid2l_con = tree.xpath('//div[@class=\"Mid2L_con\"]')\n",
    "\n",
    "        # Clean the URL to make it filename-safe\n",
    "        filename_safe_url = currenturl.replace(\":\", \"=\").replace(\"/\", \"|\")\n",
    "\n",
    "        if mid2l_con:\n",
    "            text_content_list = [f\"Title: {title_text}\"]\n",
    "            text_with_images_list = [f\"Title: {title_text}\"]\n",
    "            txt_data_list = []\n",
    "            stop_extraction = False\n",
    "\n",
    "            # First pass: gather all text and image elements\n",
    "            for element in mid2l_con[0].iter():\n",
    "                if stop_extraction:\n",
    "                    break\n",
    "\n",
    "                # If it's a text node, extract the text and tail\n",
    "                if element.text and isinstance(element.tag, str):\n",
    "                    text = element.text.strip()\n",
    "                else:\n",
    "                    text = \"\"\n",
    "\n",
    "                # Also check the 'tail' for text outside the tag\n",
    "                if element.tail:\n",
    "                    tail_text = element.tail.strip()\n",
    "                else:\n",
    "                    tail_text = \"\"\n",
    "\n",
    "                # Combine text and tail_text\n",
    "                combined_text = text + \" \" + tail_text if text or tail_text else \"\"\n",
    "\n",
    "                # Append the combined text to the list if it's not empty and does not contain certain phrases\n",
    "                if combined_text:\n",
    "                    # Check if the combined text starts with \"本文由游民星空\"\n",
    "                    if combined_text.startswith((\"本文由游民星空\", \"推荐下载\")):\n",
    "                        stop_extraction = True\n",
    "                    else:\n",
    "                        # Check if combined_text contains any of the unwanted phrases\n",
    "                        unwanted_phrases = [\n",
    "                            \"更多相关内容请关注\",\n",
    "                            \"责任编辑\",\n",
    "                            \"友情提示：\",\n",
    "                            \"本文是否解决了您的问题\",\n",
    "                            \"已解决\",\n",
    "                            \"未解决\",\n",
    "                            \"黑神话：悟空专区\",\n",
    "                            \"上一页\",\n",
    "                            \"下一页\"\n",
    "                        ]\n",
    "                        if not any(phrase in combined_text for phrase in unwanted_phrases):\n",
    "                            text_content_list.append(combined_text)\n",
    "                            text_with_images_list.append(combined_text)\n",
    "\n",
    "\n",
    "                # If it's an <img> tag\n",
    "                if element.tag == 'img' and not stop_extraction:\n",
    "                    img_src = element.get('src')\n",
    "                    img_data_src = element.get('data-src', img_src)  # Use data-src if available, otherwise fallback to src\n",
    "                    img_alt = element.get('alt', '')\n",
    "                    img_title = element.get('title', '')\n",
    "                    img_width = element.get('width', '')\n",
    "                    img_height = element.get('height', '')\n",
    "\n",
    "                    # Convert relative paths to absolute URLs\n",
    "                    img_src = urljoin(currenturl, img_data_src)\n",
    "\n",
    "                    img_src=img_src.replace('_S.jpg', '.jpg')\n",
    "\n",
    "                    # Save the raw image to a file\n",
    "                    save_image_to_file(img_src)\n",
    "\n",
    "                    # Replace the placeholder with the actual image tag\n",
    "                    img_tag = f'<img src=\"{img_src}\" alt=\"{img_alt}\" width=\"{img_width}\" height=\"{img_height}\" title=\"{img_title}\">'\n",
    "                    text_with_images_list.append(img_tag)\n",
    "\n",
    "            # Convert text_content_list to a single string\n",
    "            text_content_str = '\\n'.join(text_content_list)\n",
    "            text_with_images_list_str = '\\n'.join(text_with_images_list)\n",
    "\n",
    "            # Save content to docs folder first\n",
    "            text_only_filename = os.path.join(\"docs/rawdata/\", f\"{filename_safe_url}_text_only.txt\")\n",
    "            text_with_images_filename = os.path.join(\"docs/rawdata/\", f\"{filename_safe_url}_text_with_images.html\")\n",
    "            save_content_to_file(text_content_str, text_only_filename)\n",
    "            save_content_to_file(text_with_images_list_str, text_with_images_filename)                \n",
    "\n",
    "            # Get img data\n",
    "            img_data_list = []\n",
    "            for img_index, line in enumerate(text_with_images_list):\n",
    "                if line.startswith(\"<img\"):\n",
    "                    # Extract image attributes\n",
    "                    img_src = line.split('src=\"')[1].split('\"')[0]\n",
    "                    img_alt = line.split('alt=\"')[1].split('\"')[0]\n",
    "                    img_width = line.split('width=\"')[1].split('\"')[0]\n",
    "                    img_height = line.split('height=\"')[1].split('\"')[0]\n",
    "                    img_title = line.split('title=\"')[1].split('\"')[0]\n",
    "                    \n",
    "                    # Get Base64 encoded image content (currently returning empty string)\n",
    "                    img_base64 = get_base64_encoded_image(img_src)\n",
    "                    \n",
    "                    # Get n lines before and after the image\n",
    "                    content_before_image = []\n",
    "                    content_after_image = []\n",
    "                    \n",
    "                    # Extract n lines before the image, stop if another <img> tag is encountered\n",
    "                    for i in range(img_index-1, max(0, img_index-n)-1, -1):\n",
    "                        # if '<img' in text_with_images_list[i]:\n",
    "                        #     break\n",
    "                        content_before_image.append(text_with_images_list[i])\n",
    "                    content_before_image.reverse()\n",
    "                    \n",
    "                    # Extract n lines after the image, stop if another <img> tag is encountered\n",
    "                    for i in range(img_index+1, min(len(text_with_images_list), img_index+1+n)):\n",
    "                        # if '<img' in text_with_images_list[i]:\n",
    "                        #     break\n",
    "                        content_after_image.append(text_with_images_list[i])\n",
    "                    \n",
    "                    content_before_image_str = '\\n'.join(content_before_image)\n",
    "                    content_after_image_str = '\\n'.join(content_after_image)\n",
    "                    image_descrip_str = ''\n",
    "                    \n",
    "                    # Add the image metadata to the img_data_list\n",
    "                    img_data_list.append({\n",
    "                        \"page_title\": title_text,  # To recognize this image with the page title.\n",
    "                        \"src\": img_src,\n",
    "                        \"base64\": img_base64,  # Temporarily set to an empty string\n",
    "                        \"title\": img_title,\n",
    "                        \"alt\": img_alt,\n",
    "                        \"content_before_image\": content_before_image_str,\n",
    "                        \"image_description\": image_descrip_str,\n",
    "                        \"content_after_image\": content_after_image_str,\n",
    "                        \"url\": currenturl,  # Current page url\n",
    "                        \"type\": \"img\"\n",
    "                    })\n",
    "\n",
    "\n",
    "            txt_data_list.append({\n",
    "                    \"txt\": text_content_str,\n",
    "                    \"url\": currenturl,\n",
    "                    \"type\": \"text\"\n",
    "                })\n",
    "\n",
    "            # Save the entire text_content_str directly to mmtext.json\n",
    "            save_data_json_with_format(txt_data_list, \"docs/mmtext.json\")\n",
    "\n",
    "            # Save the image metadata to JSON as a list of objects\n",
    "            save_data_json_with_format(img_data_list, \"docs/mmimg.json\")\n",
    "\n",
    "            return f\"Content saved to files in docs and JSON files processed.\"\n",
    "        else:\n",
    "            return \"No content found with class='Mid2L_con'.\"\n",
    "    else:\n",
    "        return \"Failed to fetch content.\"\n",
    "\n",
    "# Load existing links from the JSON file\n",
    "def load_existing_links(filename):\n",
    "    \"\"\"\n",
    "    Loads existing links from the specified JSON file.\n",
    "    If the file doesn't exist, it returns an empty list.\n",
    "    \"\"\"\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'r', encoding=\"utf-8\") as json_file:\n",
    "            return json.load(json_file)\n",
    "    return []\n",
    "    \n",
    "\n",
    "\n",
    "# Global variable to track new links added across function calls\n",
    "new_link_count = 0\n",
    "\n",
    "def save_link_to_json(new_link, filename=\"docs/links.json\"):\n",
    "    \"\"\"\n",
    "    Saves the provided link to the specified JSON file.\n",
    "    If the link contains .shtml?, it removes the string after .shtml.\n",
    "    If the link already exists, it returns False.\n",
    "    If the link is new and added, it returns True.\n",
    "    Logs the added link along with the updated total count of new links.\n",
    "    \"\"\"\n",
    "    global new_link_count  # Use the global counter for new links\n",
    "\n",
    "    # Check if the new_link contains '.shtml?'\n",
    "    if \".shtml?\" in new_link:\n",
    "        new_link = new_link.split(\".shtml?\")[0] + \".shtml\"\n",
    "    \n",
    "    links = load_existing_links(filename)\n",
    "    \n",
    "    if new_link not in links:\n",
    "        links.append(new_link)\n",
    "        new_link_count += 1  # Increment the global counter for a new link\n",
    "        with open(filename, 'w', encoding=\"utf-8\") as json_file:\n",
    "            json.dump(links, json_file, indent=4, ensure_ascii=False)\n",
    "        log_message(f\"New link added to links.json: {new_link}. Total links: {new_link_count}\")\n",
    "        return True\n",
    "    else:\n",
    "        log_message(f\"Link already in links.json: {new_link}\")\n",
    "        return False    \n",
    "\n",
    "\n",
    "\n",
    "# Global variable to track new URLs added across function calls\n",
    "new_url_count = 0\n",
    "\n",
    "# Save crawled URLs to the JSON file and count added URLs\n",
    "def save_crawled_url_to_json(new_url, filename=\"docs/crawled_urls.json\"):\n",
    "    \"\"\"\n",
    "    Saves the provided URL to the specified JSON file.\n",
    "    If the URL already exists, it returns False.\n",
    "    If the URL is new and added, it returns True.\n",
    "    Logs the added URL along with the updated total count of new URLs.\n",
    "    \"\"\"\n",
    "    global new_url_count  # Use the global counter for new URLs\n",
    "\n",
    "    # Check if the file exists and load existing URLs\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'r', encoding=\"utf-8\") as json_file:\n",
    "            urls = json.load(json_file)\n",
    "    else:\n",
    "        urls = [] \n",
    "    \n",
    "    log_message(f\"-----------------------------------------------------------\")\n",
    "    \n",
    "    # Check if the URL is new\n",
    "    if new_url not in urls:\n",
    "        urls.append(new_url)\n",
    "        new_url_count += 1  # Increment the global counter for a new URL\n",
    "        with open(filename, 'w', encoding=\"utf-8\") as json_file:\n",
    "            json.dump(urls, json_file, indent=4, ensure_ascii=False)\n",
    "        log_message(f\"New crawled url added to crawled_urls.json: {new_url}. Total urls: {new_url_count}\")\n",
    "        return True \n",
    "    else:\n",
    "        log_message(f\"Url already in crawled_urls.json: {new_url}\")\n",
    "        return False\n",
    "    \n",
    "\n",
    "# Check if the link exists in the JSON file\n",
    "def check_link_in_json(new_link, filename=\"docs/links.json\"):\n",
    "    \"\"\"\n",
    "    Checks if the provided link exists in the specified JSON file.\n",
    "    If the link contains .shtml?, it removes the string after .shtml.\n",
    "    Returns True if the link is found, otherwise False.\n",
    "    \"\"\"\n",
    "    # Check if the new_link contains '.shtml?'\n",
    "    if \".shtml?\" in new_link:\n",
    "        new_link = new_link.split(\".shtml?\")[0] + \".shtml\"\n",
    "    \n",
    "    # Load existing links from the JSON file\n",
    "    links = load_existing_links(filename)\n",
    "    \n",
    "    # Return True if the link is found, otherwise False\n",
    "    if new_link in links:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "# Send request with retry mechanism\n",
    "def fetch_url_with_retries(url, max_retries=2):\n",
    "    \"\"\"\n",
    "    Attempts to fetch content from the given URL, retrying up to max_retries times.\n",
    "    If the request fails, it waits for 1 second before retrying.\n",
    "    \"\"\"\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            response = requests.get(url, timeout=3)  # Set timeout to 3 seconds\n",
    "            \n",
    "            # If the status code is 200, the request was successful, return the content\n",
    "            if response.status_code == 200:\n",
    "                log_message(f\"Success on attempt {retries + 1} for {url}\")\n",
    "                return response.content\n",
    "            \n",
    "            # If the status code is not 200, log the failure reason\n",
    "            else:\n",
    "                log_message(f\"Attempt {retries + 1} failed with status code {response.status_code}\")\n",
    "        \n",
    "        except requests.RequestException as e:\n",
    "            # Capture request exceptions like timeout or connection errors\n",
    "            log_message(f\"Attempt {retries + 1} failed with error: {e}\")\n",
    "        \n",
    "        # Increment retry count\n",
    "        retries += 1\n",
    "        \n",
    "        # Wait for 1 second before retrying\n",
    "        time.sleep(1)\n",
    "\n",
    "    # If max retries are exceeded, return None or handle the error accordingly\n",
    "    log_message(f\"Failed to fetch the URL after {max_retries} attempts.\")\n",
    "    return None\n",
    "\n",
    "# New function to crawl the webpage and its linked pages up to a given depth\n",
    "def crawl_and_extract(url, keyword=\"黑神话\", linkdepth=2):\n",
    "    \"\"\"\n",
    "    Crawls the webpage starting from the given URL, and checks for links within the page.\n",
    "    If a page contains the term specified in 'keyword' in either \"Mid2L_con\" class or in the title,\n",
    "    or in the whole HTML content, it saves the link in 'links.json'.\n",
    "    Crawls up to the given linkdepth (including the original URL).\n",
    "    \"\"\"\n",
    "    def crawl_nest(url, current_depth, max_depth, pbar):\n",
    "\n",
    "        # Check if the link has been crawled\n",
    "        if save_crawled_url_to_json(url) == False:\n",
    "            return\n",
    "\n",
    "\n",
    "        # Check if the link exists in the JSON file and it is in the max depth, if yes, just return.\n",
    "        if check_link_in_json(url) == True and current_depth == max_depth:\n",
    "            log_message(f\"Link found in links.json: {url} (Depth: {current_depth})\")\n",
    "            return\n",
    "        \n",
    "        # Fetch the page content\n",
    "        html_content = fetch_url_with_retries(url)\n",
    "        if not html_content:\n",
    "            log_message(f\"Failed to fetch content for {url} (Depth: {current_depth})\")\n",
    "            return\n",
    "        \n",
    "        # Parse the HTML using lxml\n",
    "        tree = html.fromstring(html_content)\n",
    "\n",
    "        # Check if class=\"Mid2L_con\" or title contains the keyword\n",
    "        mid2l_con_elements = tree.xpath('//div[@class=\"Mid2L_con\"]')\n",
    "        title_elements = tree.xpath('//title/text()')\n",
    "        \n",
    "        # Check if keyword exists in Mid2L_con or Title\n",
    "        mid2l_con_text = mid2l_con_elements[0].text_content() if mid2l_con_elements else \"\"\n",
    "        title_text = title_elements[0] if title_elements else \"\"\n",
    "        \n",
    "        links = []\n",
    "\n",
    "        if mid2l_con_text:\n",
    "            if keyword in mid2l_con_text or keyword in title_text:\n",
    "                if current_depth == 0: current_depth = 1   # 0 is for url without mid2l_con, so we set it to 1\n",
    "                log_message(f\"Found '{keyword}' in Mid2L_con or Title at {url} (Depth: {current_depth})\")\n",
    "                linkexist = save_link_to_json(url)  # Save the link to JSON\n",
    "                if linkexist == True:\n",
    "                    extract_text_and_images(url, tree)\n",
    "                if current_depth < max_depth:\n",
    "                    # Get all links on the page\n",
    "                    alinks = tree.xpath('//div[@class=\"Mid2L_con\"]//a[@href]/@href')\n",
    "                    links = [urljoin(url, link) for link in alinks if link.startswith(('http', '/'))]\n",
    "                    links = list(set(links))\n",
    "                    # Remove unwanted link, links starting with 'javascript:', and those ending with '.jpg' or '.png'\n",
    "                    unwanted_link = \"\" # \"https://www.gamersky.com/z/bmwukong/\"\n",
    "                    filtered_links = [link for link in links if link != unwanted_link and not link.startswith('javascript:') and not link.endswith(('.jpg', '.png'))]\n",
    "                    links = filtered_links\n",
    "                    log_message(f\"Found {len(links)} links on {url} (Depth: {current_depth}). Crawling deeper...\")\n",
    "\n",
    "        else:\n",
    "            # If not found in Mid2L_con, check the full HTML content\n",
    "            if keyword in title_text: \n",
    "                current_depth = 0\n",
    "                log_message(f\"Found '{keyword}' in full HTML at {url} (Depth: {current_depth})\")\n",
    "            # if keyword in html_content.decode('utf-8', errors='ignore'):                \n",
    "                linkexist = save_link_to_json(url)  # Save the link to JSON\n",
    "                if linkexist == True:\n",
    "                    pass\n",
    "                    # extract_text_and_images(url, tree)   # Don't extract if it is just an overview\n",
    "\n",
    "                if current_depth < max_depth:\n",
    "                    # Get all links on the page\n",
    "                    alinks = tree.xpath('//a[@href]/@href')\n",
    "                    links = [urljoin(url, link) for link in alinks if link.startswith(('http', '/'))]\n",
    "                    links = list(set(links))\n",
    "                    # Remove unwanted link, links starting with 'javascript:', and those ending with '.jpg' or '.png'\n",
    "                    unwanted_link = \"\" # \"https://www.gamersky.com/z/bmwukong/\"\n",
    "                    filtered_links = [link for link in links if link != unwanted_link and not link.startswith('javascript:') and not link.endswith(('.jpg', '.png'))]\n",
    "                    links = filtered_links\n",
    "                    log_message(f\"Found {len(links)} links on {url} (Depth: {current_depth}). Crawling deeper...\")\n",
    "            else:\n",
    "                log_message(f\"No '{keyword}' found at {url} (Depth: {current_depth})\")\n",
    "\n",
    "        \n",
    "        if current_depth < max_depth and links:\n",
    "            current_depth = current_depth + 1\n",
    "            # Recursively crawl the found links, with increased depth\n",
    "            for link in tqdm(links, desc=f\"Crawling depth {current_depth}/{max_depth}\", leave=False, position=1, dynamic_ncols=True):\n",
    "                crawl_nest(link, current_depth, max_depth, pbar)\n",
    "                pbar.update(1)\n",
    "\n",
    "\n",
    "\n",
    "    # Write the start message\n",
    "    log_message(\"=== Crawl Start ===\")\n",
    "\n",
    "    with tqdm(total=100, desc=\"Crawling\", position=0, dynamic_ncols=True) as pbar:\n",
    "        crawl_nest(url, current_depth=0, max_depth=linkdepth, pbar=pbar)\n",
    "\n",
    "    # Write the end message with two empty lines\n",
    "    log_message(\"=== Crawl End ===\\n\\n\")\n",
    "\n",
    "# Test URL and Keyword\n",
    "url = \"https://www.gamersky.com/z/bmwukong/\"\n",
    "keyword = \"黑神话\"\n",
    "\n",
    "crawl_and_extract(url, keyword=keyword, linkdepth=2) # Crawl the URL and its linked pages up to a depth\n",
    "# get_all_image_description('docs/mmimg_nim.json') # Get image description for all images in the JSON file\n",
    "\n",
    "# image_src='http://img1.gamersky.com/image2024/08/20240819_qy_372_15/image077.jpg'\n",
    "# content_before_image_str =''\n",
    "# content_after_image_str=''\n",
    "# content_before_image_str='Title: 《黑神话悟空》珍玩图鉴 珍玩获取方法及效果一览\\n2024-08-20 10:19:28 来源：游民星空[原创] 作者：瑞破受气包  我要投稿\\n第13页：特品-金棕衣 \\n展开 \\n特品-金棕衣 \\n获取方法：【 \\n可能是焦面鬼王概率掉落 】。从第三回【极乐谷-长生大道】土地庙出发，进入土地庙前方木门之后往左前方走，击败前方雪地上的焦面鬼王（超大巨人）即可获得。具体路线请参考下文。\\n<img src=\\\"http://img1.gamersky.com/image2024/08/20240819_qy_372_15/image073.jpg\\\" alt=\\\"游民星空\\\" width=\\\"\\\" height=\\\"\\\" title=\\\"\\\">\\n<img src=\\\"http://img1.gamersky.com/image2024/08/20240819_qy_372_15/image075.jpg\\\" alt=\\\"游民星空\\\" width=\\\"\\\" height=\\\"\\\" title=\\\"\\\">\\n从第三回【极乐谷-长生大道】土地庙出发，进入土地庙前方木门之后往左前方走。 '\n",
    "# content_after_image_str='继续沿路前进，在拐弯处往右走，可以看到一大片雪地，还有一个超大巨人，巨人就是焦面鬼王，击杀即可获得。 \\n<img src=\\\"http://img1.gamersky.com/image2024/08/20240819_qy_372_15/image079.jpg\\\" alt=\\\"游民星空\\\" width=\\\"\\\" height=\\\"\\\" title=\\\"\\\">\\n11 \\n12 \\n13 \\n14 \\n15 \\n16 \\n17 \\n18 \\n19 \\n20 \\n21 \\n0 \\n0 \\n文章内容导航 \\n第1页：上品-猫睛宝串 \\n第2页：上品-玛瑙罐 \\n第3页：上品-不求人 \\n第4页：上品-砗磲佩 '\n",
    "\n",
    "# get_image_description(image_src, content_before_image_str, content_after_image_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized an empty vectorstore in vectorstore/chromadb-mmgamerag\n",
      "Quantity of existing_urls: 0\n",
      "Added 5265 new text documents.\n",
      "Quantity of existing_srcs: 0\n",
      "Added 19753 new img documents.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv,find_dotenv\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.schema.document import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "load_dotenv(find_dotenv()) \n",
    "\n",
    "# Preparation of documents for RAG-------------------------\n",
    "# Vectorstore, for retrieval\n",
    "embedding_model=OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "vectorstore_path = \"vectorstore/chromadb-mmgamerag\"\n",
    "if os.path.exists(vectorstore_path):\n",
    "    print(f\"Loaded vectorstore from disk: {vectorstore_path}\")\n",
    "else:\n",
    "    # Initialize an empty vectorstore and persist to disk\n",
    "    print(f\"Initialized an empty vectorstore in {vectorstore_path}\")\n",
    "\n",
    "vectorstore = Chroma(\n",
    "                embedding_function=embedding_model,\n",
    "                persist_directory=vectorstore_path,\n",
    "                ) \n",
    "\n",
    "def add_text_documents_to_vectorstore(vectorstore, documents):\n",
    "    # Retrieve existing documents from the vectorstore\n",
    "    existing_docs = vectorstore.get()\n",
    "    \n",
    "    existing_urls = [metadata['url'] for metadata in existing_docs['metadatas']] #???metadata\n",
    "    print(f\"Quantity of existing_urls: {len(existing_urls)}\")\n",
    "    # print(existing_urls)\n",
    "    # Filter out documents that already exist based on URL\n",
    "    new_documents = [doc for doc in documents if doc.metadata[\"url\"] not in existing_urls]\n",
    "\n",
    "    if new_documents:\n",
    "        vectorstore.add_documents(new_documents)\n",
    "        # vectorstore.persist()  # Persist the vectorstore after adding documents\n",
    "        print(f\"Added {len(new_documents)} new text documents.\")\n",
    "    else:\n",
    "        print(\"No new text documents to add.\")\n",
    "\n",
    "\n",
    "def add_img_documents_to_vectorstore(vectorstore, documents):\n",
    "    # Retrieve existing documents from the vectorstore\n",
    "    existing_docs = vectorstore.get()\n",
    "    \n",
    "    # Use `get` to avoid KeyError if some metadata does not have 'src'\n",
    "    existing_srcs = [metadata.get('src') for metadata in existing_docs['metadatas'] if 'src' in metadata]\n",
    "    print(f\"Quantity of existing_srcs: {len(existing_srcs)}\")\n",
    "    # print(existing_srcs)\n",
    "    \n",
    "    # Filter out documents that already exist based on src\n",
    "    new_documents = [doc for doc in documents if doc.metadata.get(\"src\") not in existing_srcs]\n",
    "\n",
    "    if new_documents:\n",
    "        vectorstore.add_documents(new_documents)\n",
    "        # vectorstore.persist()  # Persist the vectorstore after adding documents\n",
    "        print(f\"Added {len(new_documents)} new img documents.\")\n",
    "    else:\n",
    "        print(\"No new img documents to add.\")\n",
    "\n",
    "\n",
    "def add_txt_img():\n",
    "    txt_data_list = []\n",
    "    img_data_list = []\n",
    "\n",
    "    # Directly load and assign to txt_data_list from mmtext.json\n",
    "    with open('docs/mmtext.json', 'r', encoding='utf-8') as text_file:\n",
    "        txt_data_list = json.load(text_file)  # Assuming the JSON structure matches the required format\n",
    "\n",
    "    # Directly load and assign to img_data_list from mmimg.json\n",
    "    with open('docs/mmimg.json', 'r', encoding='utf-8') as img_file:\n",
    "        img_data_list = json.load(img_file)  # Assuming the JSON structure matches the required format\n",
    "\n",
    "    # Add texts\n",
    "    mmtexts = [\n",
    "        Document(page_content=item['txt'], metadata={\"url\": item['url'], \"type\": item['type']})\n",
    "        for item in txt_data_list\n",
    "    ]\n",
    "\n",
    "    # Add documents and save to vectorstore\n",
    "    add_text_documents_to_vectorstore(vectorstore, mmtexts)\n",
    "\n",
    "\n",
    "    # Add imgs\n",
    "    mmimgs = [\n",
    "        Document(\n",
    "            page_content=\"\\npage_title:\\n\" + item['page_title'] + \"\\n\\ncontent_before_image:\\n\" + item['content_before_image'] + \"\\n\\nimage_description:\\n\" + item['image_description'] + \"\\n\\ncontent_after_image:\\n\" + item['content_after_image'] + '\\n',  \n",
    "            metadata={\"url\": item['url'], \"type\": item['type'], \"src\": item['src']}\n",
    "        )\n",
    "        for item in img_data_list  # Iterate over each item in img_data_list\n",
    "    ]\n",
    "\n",
    "    # Add documents and save to vectorstore\n",
    "    add_img_documents_to_vectorstore(vectorstore, mmimgs)\n",
    "\n",
    "add_txt_img() # Add texts and images to vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "\n",
    "# retrieved_docs = retriever.invoke(\"猫睛宝串\")\n",
    "# retrieved_docs\n",
    "\n",
    "retrieved_docs = vectorstore.similarity_search_with_relevance_scores(query=\"君子牌\", k=5, filter={\"type\": \"text\"})\n",
    "\n",
    "# Iterate over retrieved_docs and extract the url, page_content, and score\n",
    "for doc, score in retrieved_docs:\n",
    "    url = doc.metadata.get('url', 'No URL found')  # Extract the URL from the metadata\n",
    "    type = doc.metadata.get('type', '') \n",
    "    page_content = doc.page_content  # Get the page content\n",
    "    # print(f\"URL: {url}\\nContent: {page_content}\\nScore: {score}\\nType: {type}\\n\")\n",
    "\n",
    "retrieved_docs = vectorstore.similarity_search_with_relevance_scores(query=\"君子牌\", k=5, filter={\"type\": \"img\"})\n",
    "\n",
    "# Iterate over retrieved_docs and extract the url, page_content, and score\n",
    "for doc, score in retrieved_docs:\n",
    "    url = doc.metadata.get('url', 'No URL found')  # Extract the URL from the metadata\n",
    "    type = doc.metadata.get('type', '') \n",
    "    src = doc.metadata.get('src', '') \n",
    "    page_content = doc.page_content  # Get the page content\n",
    "    print(f\"URL: {url}\\nSRC: {src}\\nContent: {page_content}\\nScore: {score}\\nType: {type}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Q&A with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display, Image\n",
    "\n",
    "mmgamellm = ChatOpenAI(name=\"MMGameRag\", model_name=\"gpt-4o-mini\", temperature=0.6, streaming=True)\n",
    "\n",
    "def format_docs(docs_with_scores):\n",
    "    \"\"\"\n",
    "    Formats the retrieved documents into a string with their content, URL, and score,\n",
    "    and lists them in order with numbering.\n",
    "    \"\"\"\n",
    "    formatted_docs = []\n",
    "    \n",
    "    # Iterate over the documents and their associated scores\n",
    "    for i, (doc, score) in enumerate(docs_with_scores, 1):  # Enumerate to add numbering starting from 1\n",
    "        imgsrc = doc.metadata.get('src', '')\n",
    "        if imgsrc: # Image\n",
    "            formatted_doc = (\n",
    "                f\"{i}.\\n\"\n",
    "                f\"Image Content:\\n{doc.page_content}\\n\"  # Content of the document\n",
    "                f\"Page Url: {doc.metadata.get('url', '')}\\n\"  # Assuming URL is stored in metadata\n",
    "                f\"Image Src: {doc.metadata.get('src', '')}\\n\"  # Assuming URL is stored in metadata\n",
    "                f\"Score: {score}\\n\"  # Similarity score for the document\n",
    "            )\n",
    "        else:  # Text\n",
    "            formatted_doc = (\n",
    "                f\"{i}.\\n\"\n",
    "                f\"Text Content:\\n{doc.page_content}\\n\"  # Content of the document\n",
    "                f\"Page Url: {doc.metadata.get('url', '')}\\n\"  # Assuming URL is stored in metadata\n",
    "                f\"Score: {score}\\n\"  # Similarity score for the document\n",
    "            )\n",
    "        formatted_docs.append(formatted_doc)  # Add formatted document to the list\n",
    "    \n",
    "    return \"\\n\".join(formatted_docs)  # Join all formatted documents into a single string\n",
    "\n",
    "# Prompt for code generation\n",
    "prompt_template = \"\"\"你是《黑神话：悟空》这款游戏的AI助手，根据Question和Context专门为玩家提供详尽的游戏攻略并以Markdown的格式输出.请注意：\n",
    "1. 在Image中找到与Question和Answer最相关的图像。每个Image都有Text before image，Image descriptioin和Text after image，可以用来判断这个Image应该被插入到与文本答案最匹配的上下文的哪个段落当中。格式如下：\n",
    "    \n",
    "    文本答案段落\n",
    "    [![](图像1的Src)](图像1的Url)\n",
    "    文本答案段落\n",
    "    [![](图像2的Src)](图像2的Url)\n",
    "    文本答案段落\n",
    "    ...\n",
    "\n",
    "2. 在输出答案的最后，根据问题找到context中的最相关的几个参考文档，并列出Url链接，以供用户参考原始文档。\n",
    "\n",
    "Question: \n",
    "{question}\n",
    "\n",
    "Context: \n",
    "{context}\n",
    "\n",
    "Image:\n",
    "{image}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt_code = ChatPromptTemplate.from_template(prompt_template)\n",
    "\n",
    "chain = (\n",
    "    prompt_code\n",
    "    | mmgamellm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "gamer_question = \"黑神话一共有多少上品珍宝？举几个例子\"\n",
    "context_retrieval = format_docs(vectorstore.similarity_search_with_score(query=gamer_question, k=5, filter={\"type\": \"text\"}))\n",
    "# print(context_retrieval + \"\\n------------------------\\n\")\n",
    "img_retrieval = format_docs(vectorstore.similarity_search_with_score(query=gamer_question, k=5, filter={\"type\": \"img\"}))\n",
    "# print(img_retrieval + \"\\n------------------------\\n\")\n",
    "result = chain.invoke({\n",
    "    \"question\": gamer_question, \n",
    "    \"context\": context_retrieval,\n",
    "    \"image\": img_retrieval\n",
    "})\n",
    "\n",
    "\n",
    "display(Markdown(result))\n",
    "# display(Image(url=\"http://img1.gamersky.com/image2024/08/20240819_qy_372_15/image001_S.jpg\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temp --  Get page title for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items: 100%|██████████| 19753/19753 [46:39<00:00,  7.05it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titles have been updated and page_title is now the first attribute in the JSON file.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "from lxml import html\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to clean and format the title text\n",
    "def clean_title_text(title_text):\n",
    "    \"\"\"\n",
    "    Clean and format the extracted title text by removing empty lines and extra spaces.\n",
    "    \"\"\"\n",
    "    lines = [line.strip() for line in title_text.splitlines() if line.strip()]\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# Read the first 10 items from the JSON file\n",
    "with open('docs/mmimg.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Loop through the first 10 items with tqdm progress bar\n",
    "# for i, item in tqdm(enumerate(data[:10]), total=10, desc=\"Processing items\"):\n",
    "for i, item in tqdm(enumerate(data), total=len(data), desc=\"Processing items\"):\n",
    "    url = item.get('url', '')  # Get the URL from the item\n",
    "    if not url:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # Fetch the HTML content from the URL\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check for HTTP errors\n",
    "        html_content = response.content\n",
    "        \n",
    "        # Parse the HTML using lxml\n",
    "        tree = html.fromstring(html_content)\n",
    "\n",
    "        title_text = ''\n",
    "        # Extract title from div with class \"Mid2L_tit\"\n",
    "        mid2L_tit_elements = tree.xpath('//div[@class=\"Mid2L_tit\"]')\n",
    "        if mid2L_tit_elements:\n",
    "            title_text = clean_title_text(mid2L_tit_elements[0].text_content())\n",
    "        else:\n",
    "            # If no title in Mid2L_tit, use the page's main title\n",
    "            title_elements = tree.xpath('//title')\n",
    "            if title_elements:\n",
    "                title_text = clean_title_text(title_elements[0].text_content())\n",
    "\n",
    "        # Reorder dictionary to have page_title as the first attribute\n",
    "        # First, remove page_title if it exists\n",
    "        if 'page_title' in item:\n",
    "            del item['page_title']\n",
    "        \n",
    "        # Create a new ordered dict with page_title as the first item\n",
    "        ordered_item = OrderedDict([('page_title', title_text)] + list(item.items()))\n",
    "\n",
    "        # Replace the old item with the newly ordered item\n",
    "        data[i] = ordered_item\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {url}: {e}\")\n",
    "\n",
    "# Save the modified JSON data back to the file\n",
    "with open('docs/mmimg.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Titles have been updated and page_title is now the first attribute in the JSON file.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get titles \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items: 100%|██████████| 5430/5430 [00:00<00:00, 279479.58it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define function to extract and write titles with progress display\n",
    "def extract_titles():\n",
    "    \"\"\"\n",
    "    Reads each item's 'txt' in docs/mmtext.json, processes the titles based on URL conditions,\n",
    "    and writes the result to docs/titles.json with a progress bar.\n",
    "    \"\"\"\n",
    "    input_path = \"docs/mmtext.json\"\n",
    "    output_path = \"docs/titles.json\"\n",
    "    titles = []\n",
    "\n",
    "    # Read input JSON file\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        items = json.load(f)\n",
    "    \n",
    "    # Process each item with tqdm progress bar\n",
    "    for item in tqdm(items, desc=\"Processing items\"):\n",
    "        url = item.get(\"url\", \"\")\n",
    "        lines = item.get(\"txt\", \"\").splitlines()\n",
    "        title_data = {}\n",
    "\n",
    "        if \"www.gamersky.com/handbook\" in url:\n",
    "            # Remove specific prefix and suffix\n",
    "            title_str = lines[0].replace(\"Title: 《黑神话悟空》\", \"\").replace(\"-游民星空 GamerSky.com\", \"\")\n",
    "            # Split by underscore\n",
    "            if \"_\" in title_str:\n",
    "                title, subtitle = title_str.split(\"_\", 1)\n",
    "            else:\n",
    "                title, subtitle = title_str, \"\"\n",
    "            title_data = {\"title\": title, \"subtitle\": subtitle, \"class\": \"攻略\"}\n",
    "\n",
    "        elif \"www.gamersky.com/news\" in url:\n",
    "            # Remove specific prefix and suffix\n",
    "            title = lines[0].replace(\"Title: \", \"\").replace(\" _ 游民星空 GamerSky.com\", \"\")\n",
    "            title_data = {\"title\": title, \"class\": \"新闻\"}\n",
    "\n",
    "        elif \"down.gamersky.com/\" in url:\n",
    "            # Remove specific prefix and suffix\n",
    "            title = lines[0].replace(\"Title: \", \"\").replace(\"_黑神话：悟空下载 - 游民星空下载中心\", \"\")\n",
    "            title_data = {\"title\": title, \"class\": \"下载\"}\n",
    "\n",
    "        if title_data:\n",
    "            titles.append(title_data)\n",
    "    \n",
    "    # Write output JSON file\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(titles, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Execute the function\n",
    "extract_titles()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
